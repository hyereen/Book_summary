

# ADP 필기 데이터 분석 전문가 올패키지

# 1~3과목
p.72 1980년대 기업내부 데이터베이스
- OLPT On-Line Transaction Processing: 호스트 컴퓨터가 데이터베이스를 액세스하고, 바로 처리결과를 돌려보내는 형태
- OLAP On-Line Analytical Processing: 다양한 비즈니스 관점에서 쉽고 빠르게 다차원적인 데이터에 접근하여 의사 결정에 활용할 수 있는 정보를 얻을 수 있게 해주는 기술

p.75 분야별 데이터베이스 
- 금융 부문
  - EAI Enterprise Application Integration: 기업 내 상호 연관된 모든 애플리케이션을 유기적으로 연동하여 필요한 정보를 중앙 집중적으로 통합, 관리, 사용할 수 있는 환경을 구현한 것
- 유통 부문
  - KMS Knowledge Management System: 지식관리 시스템, 기업의 환경이 물품을 주로 생산하던 산업사회에서 지적 재산의 중요성이 커지는 지식사회로 이동함에 따라 기업 경영을 지식이라는 관점에서 새롭게 조명하는 접근 방식
  - RFID Radio Frequency: 주파수를 이용해 ID를 식별하는 시스템으로 일명 전자태그로 불림

p.128 ETL 개요
- ETL 기능
  - Extraction 추출: 하나 또는 그 이상의 데이터 원천들로부터 데이터 획득
  - Trasnformation 변형: 데이터 클렌징, 형식 변환, 표준화, 통합 또는 다수 애플리케이션에 내장된 비즈니스 룰 적용 등
  - Load 적재: 변형 단계의 처리가 완료된 데이터를 특정 목표 시스템에 적재
- MPP Massivley Parallel Processing: 프로그램을 여러 부분으로 나누어 여러 프로세스가 각 부분을 동시에 수행시키는 것으로 대규모 병렬 처리를 의미

p.130 ODS 구성
- ODS Operational Data Store: 데이터에 대한 추가 작업을 위해 다양한 데이터 원천들로부터 데이터를 추출, 통합한 데이터베이스
- ODS 내 데이터는 향후 비즈니스 지원을 위해 타 정보 시스템으로 이관되거나 다양한 보고서 생성을 위해 데이터 웨어하우스로 이관됨
- ODS는 일반적으로 실시간 또는 실시간 근접 트랜잭션 데이터 혹은 가격 등의 원자성(개별성)을 지닌 하위 수준 데이터들을 저장하기 위해 설계됨
- ODS는 현재 혹은 비교적 최근의 데이터를 저장하기 위해 설계된다는 것을 기억해야 함
- ODS 구성 단계
  - 인터페이스 단계: 다양한 데이터 원천으로부터 데이터를 획득
  - 데이터 스테이징 단계: 작업 일정이 통제되는 프로세스들에 의해 데이터 원천들로부터 트랜잭션 데이터들이 추출되어 하나 또는 그 이상의 스테이징 테이블들에 저장
  - 데이터 프로파일링 단계: 범위, 도메인, 유일성 확보 등의 규칙을 기준으로 데이터 품질 점검
  - 데이터 클렌징 단계: 클렌징 ETL 프로세스들로 데이터 프로파일링 단계에서 식별된 오류 데이터들을 수정
  - 데이터 인티그레이션 단계: 수정 완료한 데이터를 ODS 내의 단일 통합 테이블에 적재
  - 익스포트 단계: 통합된 데이터에 대해 익스포트 규칙과 보안 규칙을 반영한 익스포트 ETL 기능을 수행해 익스포트 테이블 생성 후 다양한 DBMS 클라이언트 또는 데이터 마트 등에 적재

p.135 데이터 웨어하우스 테이블 모델링 기법
- 스타 스키마 = 조인 스키마
  - 데이터 웨어하우스 스키마 중 가장 단순
  - 단일 Fact 테이블을 중심으로 다수의 Dimension 테이블로 구성됨
  - 전통적인 관계형 데이터베이스를 통해 다차원의 데이터베이스 기능 구현 가능
  - 스타스키마의 Fact 테이블은 보통 제3정규형으로 모델링
  - Dimension 테이블은 보통 비정규화된 제2정규형으로 모델링하는 것이 일반적
  - 장점: 복잡도가 낮아 이해하기 쉽고 쿼리 작성이 용이함, 조인 테이블 개수가 적음
  - 단점: Dimension 테이블의 비정규화에 따른 데이터 중복으로 인해 테이블로 데이터를 적재할 때 상대적으로 많은 시간이 소요됨
- 스노우 플레이크 스키마
  - 스타스키마의 Dimension 테이블을 제3정규형으로 정규화한 형태
  - 장점: 데이터 중복이 제거돼 데이터 적재 시 시간이 단축됨
  - 단점: 스타 스키마에 비해 스키마 구조의 복잡성이 증가하므로 조인 테이블의 개수가 증가하고 쿼리 작성 난이도 상승

p.137 CDC
- CDC Change Data Capture
  - 데이터베이스 내 데이터에 대한 변경을 식별해 필요한 후속처리(데이터 전송/공유 등)를 자동화하는 기술 또는 설계 기법이자 구조
  - 실시간 또는 근접 실시간 데이터 통합을 기반으로 하는 데이터 웨어하우스 및 기타 데이터 저장소 구축에 폭 넓게 활용됨
  - 스토리지 하드웨어 계층에서부터 애플리케이션 계층에 이르기까지 다양한 계층에서 다양한 기술을 통해 구현될 수 있음
  - 단일 정보 시스템 내 다수의 CDC 메커니즘이 구현돼 동작될 수 있음
- 구현 기법
  - Time Stamp on Rows
    - 변경이 반드시 인지되어야 하는 테이블 내 마지막 변경 시점을 기록하는 타임스탬프 칼럼을 둠
    - 마지막 변경 타임스탬프 값보다 더 최근의 타임스탬프 값을 갖는 레코드를 변경된 것으로 식별하는 기법
  - Version Numbers on Rows
    - 변경이 반드시 인지되어야 하는 테이블 내 해당 레코드의 버전을 기록하는 칼럼을 둠
    - 기 식별된 레코드 버전보다 더 높은 버전을 보유한 레코드를 변경된 것으로 식별하는 기법
    - 레코드들의 최신 버전을 기록, 관리하는 참조 테이블을 함꼐 운영하는 것이 일반적
  - Status on Rows
    - 타임스탬프 및 버전 넘버 기겁에 대한 보완 용도로 활용
    - 데이터 변경 여부를 T/F의 Boolean 값으로 저장하는 칼럼의 상태 값을 기반으로 변경 여부를 판단하는 기법
    - 더 높은 버전 넘버 또는 더 최근 갱신 타임스탬프를 보유한 레코드에 대한 변경 여부 판단을 사람이 직접 결정할 수 있도록 유보하는 등의 업무 규칙 적용 가능
  - Time/Version/Status on Rows
    - 타임스탬프, 버전 넘버, 상태 값의 세 가지 특성을 모두 활용하는 기법
    - 정교한 쿼리 생성에 활용해 개발 유연성 제공
  - Triggers on Tables
    - 데이터베이스 트리거를 활용해 사전에 등록된 다수 대상 시스템에 변경 데이터를 배포하는 형태로 CDC를 구현하는 기법
    - 데이터에비스 트리거는 시스템 관리 복잡도 증가, 변경 관리의 어려움, 확장성 감소를 유발하는 등 전반적인 시스템 유지보수성을 저하시키는 특성이 있어 사용에 주의를 요함
  - Event Programming 
    - 데이터 변경 식별 기능을 애플리케이션에 구현
    - 애플리케이션 개발 부담과 복잡도 증가
    - 다양한 조건에 의한 CDC 메커니즘 구현 가능
  - Log Scanner on Database
    - DBMS에서 제공하는 트랜잭션 로그에 대한 스캐닝 및 변경 내역에 대한 해석을 통해 CDC 메커니즘을 구현하는 기법
    - DBMS 마다 트랜잭션 로그 관리 메커니즘이 상이해 다수의 이기종 데이터베이스를 활용하는 환경에서 적용 시 작업 규모가 증가될 수 있어 주의
    - 장점: 데이터베이스와 사용 애플리케이션에 대한 영향도 최소화, 변경 식별 지연시간 최소화, 트랜잭션 무결성에 대한 영향도 최소화, 데이터베이스 스키마 변경 불필요
- CDC 구현 방식
  - 푸시 방식: 데이터 원천에서 변경을 식별하고 대상 시스템에 변경 데이터를 적재해주는 방식
  - 풀 방식: 대상 시스템에서 데이터 원천을 정기적으로 살펴보고, 필요 시 데이터를 다운로하는 방식

p.139 EAI
- EAI Enterprise Application Integration
  - 비즈니스 프로세스를 중심으로 기업 내 각종 애플리케이션간의 상호 연동이 가능하도록 통합하는 솔루션
  - 기업 내 또는 기업 간 상호 이질적 정보 시스템들의 데이터를 연계함으로써 상호 융화 내지 동기화돼 동작하도록 하는 것
  - 산재 되어 있는 애플리케이션을 프로세스 및 메시지 차원에서 통합 및 관리
  - EAI를 통해 비즈니스 프로세스를 자동화하고 실시간으로 통합 연계
  - ETL은 배치 프로세스 중심, EAI는 실시간 혹은 근접 실시간 처리 중심
- 데이터 연계 방식
  - Point to Point
    - 기존 데이터 연계 방식
    - 기준 마스터 데이터의 통합과 표준화가 불가능
    - 복잡한 데이터 연계 경로 발생으로 인해 유지보수성 저하, 관리비용 상승
  - Hub and Spoke
    - EAI 데이터 연계 방식
    - 가운데 지점에 허브 역할을 하는 브로커를 둠
    - 연결 대상 노드들의 데이터 연계 요구를 중계해줌으로써 노드 간 연결 개수 및 구조를 단순화하는 방식
    - ETL/CDC는 운영 데이터와 분석을 위한 데이터베이스가 구분되지만, EAI는 다수 정보 시스템의 데이터를 중앙의 허브가 연계하고 통합하는 기법
    - 각 연결의 대상이 되는 노드들은 Spoke에 해당함
- EAI 구성요소
  - 어댑터 Adapter: 각 정보 시스템과 EAI허브(엔진)간의 연결성 확보
  - 버스 BUS: 어댑터를 매개로 연결된 각 정보시스템들 간의 데이터 연동 경로
  - 브로커 Broker: 데이터 연동 규칙을 통제
  - 트랜스포머 Transformer: 데이터 형식 변환을 담당
- EAI 구현 유형
  - Mediation(Intra-Communication)
    - EAI 엔진이 중개자로 동작, 특정 정보 시스템 내의 데이터 신규 생성 및 갱신 등 이벤트 발생을 식별하여 미리 약속된 정보 시스템에 해당 내용을 전달
    - Publish/Subscribe Model이라고 부름
  - Federation(Inter-Communication)
    - EAI 엔진이 외부 정보시스템으로부터 데이터 요청들을 일괄적으로 수령해 필요한 데이터 전달
    - Request/Reply Model이라고 부름
- EAI 활용 효과
  - 정보 시스템 개발 및 유지보수 비용 절감
  - 기업 정보 시스템의 지속적 발전 기반 확보
  - 관련 데이터 동기화 등을 위한 데이터 표준화 기반 제공
- EAI와 ESB 비교

| 구분 | EAI                                          | ESB                     |
|-----|----------------------------------------------|------------------------|
 |기능  | 미들웨어(Hub)를 이용하여 비즈니스 로직을 중심으로 머플리케이션을 통합, 연계 | 미들웨어(Bus)를 이용하여 서비스를 중심으로 시스템을 유기적으로 연계|
 | 통합관점 | 어플리케이션 | 프로세스 |
 | 로직연동 | 개별 어플리케이션에서 수행 | ESB에서 수행|
 | 아키텍처 | 단일 접점인 허브시스템을 이용한 중앙집중식 연결구조 | 버스 형태의 느슨하고 유연한 연결구조 |

p.164 분산 파일 시스템
- 분산 데이터 저장 기술은 분산 파일시스템, 클러스터, 데이터베이스, NoSQL로 구분
  
p.164 구글 파일 시스템 GFS Google File System
- 구글의 대규모 클러스터 서비스 플랫폼의 기반이 되는 시스템
- 일반적 파일 시스템에서의 클러스터 및 섹터와 유사하게 고정된 크기(64mb)의 청크(chunk)들로 나누고, 각 chunk에 대한 여러 개의 복제본과 chunk를 청크 서버에 분산, 저장
- 청크서버들은 데이터를 자동으로 복사하여 저장하고, 주기적으로 청크 서버의 상태를 마스터에게 전달
- GFS에서는 chunk의 기본 크기를 64mb로 지정하고, 트리 구조가 아닌 해시 테이블 구조 등을 사용함으로써 메모리상에서 보다 효율적인 메타데이터의 처리를 지원
- chunk는 마스터에 의해 생성/삭제될 수 있고 유일한 식별자에 의해 구별됨
- 구성요소
  - 클라이언트
    - 파일 읽기/쓰기 동작을 요청하는 애플리케이션
    - POSIX 인터페이스 지원 X
    - 여러 클라이언트에서 원자적인 데이터 추가 연산을 지원하기 위한 인터페이스를 지원
  - 마스터
    - 단일 마스터 구조로 파일 시스템의 Name Space, 파일과 chunk의 매핑정보, 각 chunk가 저장된 청크서버들의 위치 정보 등에 해당하는 모든 메타데이터를 메모리상에서 관리
    - 주기적으로 수집되는 청크서버의 하트비트 메시지를 이용하여 chunk들의 상태에 따라 chunk를 재복제하거나 재분산하는 것과 같은 회복 동작 수행
    - 하나의 청크서버를 primary로 지정하여 복제본의 갱신 연산을 일관되게 처리할 수 있도록 보장

p.169 러스터 Lustre
- 틀러스터 파일 시스템에서 개발한 객체 기반의 클러스터 파일 시스템
- 계층화된 모듈 구조로 TCP/IP, 인피니밴드, 미리넷과 같은 네트워크 지원
- 구성 요소
  - 클라이언트 파일 시스템: 리눅스에서 설치할 수 있는 파일 시스템, 메타데이터 서버와 객체 저장 서버들과 통신하면서 클라이언트 응용에 파일 시스템 인터페이스 제공
  - 메타데이터 서버: 파일 시스템의 이름 공간과 파일에 대한 메타데이터를 관리
  - 객체 저장 서버: 파일데이터를 저장하고, 클라이언트로부터 객체의 입출력 요청을 처리, 데이터는 세그먼트라는 작은 데이터 단위로 분할해서 복수의 디스크 장치에 분산시키는 스트라이핑 방식으로 분산, 저장
- 구동 방식
  - 유닉스 시맨틱을 제공하면서 파일 메타데이터에 대해서는 라이트백 캐시를 지원
  - 클라이언트에서 메타데이터 변경에 대한 갱신 레코드를 생성하고 나중에 메타데이터 서버에 전달
    - 라이트백 캐시 Write Back Cache: 기본적으로 데이터를 캐시에만 저장하고, 어쩔 수 없이 캐시영역에서 밀려나는 경우에 하위 저장소에서 저장하는 데이터 갱신 방식

p.170 데이터베이스 클러스터
- 하나의 데이터베이스를 여러 개의 서버 상에 구축하는 것을 의미
- 데이터를 통합할 때, 성능과 가용성의 향상을 위해 데이터베이스 차원의 파티셔닝 또는 클러스터링을 이용함
- 데이터베이스 파티셔닝 구현 효과: 병렬처리, 고가용성, 성능향상
- 데이터베이스 클러스터 구분
  - 데이터베이스 시스템 구성 형태에 따라 단일 서버 내 파티셔닝 / 다중 서버 사이 파티셔닝
  - 리소스 공유 관점에 따라 공유 디스크와 무공유 디스크로 구분
    - 무공유 디스크 Shared Nothing
      - 무공유 클러스터에서 각 데이터베이스 인스턴스는 자신이 관리하는 데이터 파일을 자신의 로컬 디스크에 저장하며, 이 파일들은 노드 간에 공유하지 않음
      - 각 인스턴스나 노드는 완전히 분리된 데이터의 서브 집합에 대한 소유권을 가지고 있음, 각 데이터는 소유권을 갖고 있는 인스턴스가 처리
      - Oracle RAC(Real Application Cluster)를 제외한 대부분의 데이터베이스 클러스터가 무공유 방식 채택
      - 장점: 노드 확장 제한 없음
      - 단점: 각 노드에 장애가 발생할 경우를 대비해 별도의 폴트톨러런스를 fault-tolerance(시스템에 고장이 발생하더라도 모든 기능 혹은 기븡의 일부를 기존과 같이 유지하는 기술)를 구성해야 함
    - 공유 디스크 Shared Disk
      - 공유 디스크 클러스터에서 모든 데이터베이스 인스턴스 노드들은 데이터 파일을 공유하고 각 인스턴스는 모든 데이터 접근할 수 있음
      - 데이터를 공유하려면 SAN(Storage Area Network)과 같은 네트워크가 반드시 있어야 함
      - 모든 노드가 데이터를 수정할 수 있기 때문에 노드 간의 동기화 작업 수행을 위한 별도의 커뮤니케이션 채널 필요
      - 장점: 높은 수준의 폴트 톨러런스를 제공하므로 클러스터를 구성하는 노드 중 하나의 노드만 살아 있어도 서비스 가능
      - 단점: 클러스터가 커지면 디스크 영역에서 병목현상 발생

p.184 분산 컴퓨팅 기술
- 맵리듀스 MapReduce
  - 구글에서 분산 병렬 컴퓨팅을 이용하여 대용량 데이터를 처리하기 위한 목적으로 제작한 소프트웨어 프레임워크
  - 분할 정복 방식으로 대용량 데이터를 병렬처리할 수 있는 프로그래밍 모델
    - Divide and Conquer: 해결하고자 하는 문제를 성질이 같은 여러 부분으로 나누어 해결한 뒤, 원래 문제의 해를 구하는 방식
  - 프로그래밍 모델: Map과 Reduce
    - Map: Key와 Value의 쌍으로 입력받음
    - Reduce: Map함수를 거친 Key와 Value쌍이 프레임워크에 의해 Reduce에 전송되고 Reduce함수를 통해 최종 Output으로 산출됨
  - 모델 적용 적합성
    - 적합: 분산 Grep이나 빈도 수 계산 등의 작업
    - 부적합: 정렬과 같은 작업

p.192 병렬 쿼리 시스템
- 직접 코딩하지 않고도 쉽고 빠르게 적용할 수 있도록 스크립트나 쿼리 인터페이스를 통해 병렬처리할 수 있는 시스템
- 구글 Sawzall: 맵리듀스를 추상화한 최초의 스크립트 형태 병렬 쿼리 언어

p.200 클라우드 컴퓨팅
- 동적으로 확장할 수 있는 가상화 자원들을 인터넷으로 서비스하는 기술을 의미
- AWS EMR: 하둡을 온디맨드로 이용할 수 있는 클라우드 서비스
- 서버 가상화
  - 인프라 기술들 중 가장 기반이 되는 기술
  - 정의: 물리적인 서버와 운영체제 사이에 적절한 계층을 추가해 서버를 사용하는 사용자에게 물리적인 자원은 숨기고 논리적인 자원만을 보여주는 기술
  - 특징: 하나의 서버에서 여러 개의 애플리케이션, 미들웨어, 운영체제들이 서로 영향을 미치지 않으면서 동시에 사용할 수 있도록 해줌

p.202 CPU 가상화
- 하이퍼바이저 Hypervisor
  - 의미: 호스트 컴퓨터에서 다수의 운영체제를 동시에 실행하도록 하기 위한 논리적인 플랫폼
  - 물리적 서버 위에 존재하는 가상화 레이어를 통해 운영체제를 수행하는 데 필요한 하드웨어 환경을 가상으로 만들어줌
  - 서버 가상화 기술의 핵심으로 x86 계열 서버 가상화에서는 소프트웨어 기반으로 하이퍼바이저를 구성
  - 이를 통해 사용자는 추가 하드웨어 구밉 없이 새로운 운영체제 설치, 애플리케이션의 테스팅 및 업그레이터를 동일한 물리적 서버에서 동시 수행 가능
  - 기능
    - 하드웨어 환경 에뮬레이션
    - 실행환경 격리
    - 시스템 자원 할당
    - 소프트웨어 스택 보존
  - 위치와 기능에 따른 분류
    - 베어메달 하이퍼바어지 Bare-metal
      - 하드웨어와 호스트 운영체제 사이에 위치
      - 다시 반 가상화 Para Virtualization과 완전 가상화 Full Virtualization으로 구분 -> Privileged 명령어를 어떻게 처리하느냐를 기준으로 분류한 것 
    - 호스트 기반 하이퍼 바이저 Hosted
       - 호스트 운영체제와 게스트 운영체제 사이에 위치

p.212 I/O 가상화
- 하나의 물리적인 장비에 여러 개의 가상머신이 실행되고 있는 상황에서 가장 문제되는 것은 I/O의 병목현상
- CPU 자원의 파텨서닝만으로는 가상화 기술을 제대로 활용할 수 없으며, I/O자원의 공유 및 파티셔닝이 필요함
- 하나의 물리적인 머신에서 운영되는 가상머신 간에도 통신이 이루어져야 하며, 이를 위해 가상 디스크 어댑터, 가상 이더넷 어댑터, 공유 이더넷 어댑터 등과 같은 기술들이 사용됨
  - 이더넷 Ethernet: LAN에 사용되는 네트워크 모델, 하나의 버스 네트워크에 1042 개의 노트를 연결할 수 있는 근거리 통신망 하드웨어, 프로토콜, 케이블 표준
- 가상 이더넷
  - 대표적인 I/O 가상화 기술, 가상화 기능 중에서 물리적으로 존재하지 않는 자원을 만들어내는 에뮬레이션 기능
  - 별도의 물리적 어댑터와 케이블을 사용하지 않고도 네트워크 이중화, 네트워크 안정적 단절 등의 효과
- 공유 이더넷 어댑터
  - 여러 개의 가상 머신이 물리적인 네트워크 카드를 공유할 수 있게 하며, 공유된 물리적 카드를 통해서 외부 네트워크와 통신 가능
  - 이 경우도 하나의 자원을 여러 가상머신이 공유하기 때문에 발생하는 병목현상은 피할 수 없음
- 가상 디스크 어댑터
  - 한 대의 서버가 여러 개의 가상머신을 구성할 경우 외장 디스크를 사용할 수 있게 해주는 파이버 채널 어댑터와 같은 I/O 어댑터의 부족 문제를 해결하기 위한 것
  - 가상화된 환경에서 가낭 디스크를 이용해 가상머신이 디스크 자원을 획득하는 방법 -> 내장 디스크, 외장디스크

p.279 데이터 거버넌스 체계 수립
- 데이터 거버넌스: 전사 차원의 모든 데이터에 대해 정책, 지침, 표준화, 운영 조직 및 책임 등의 표준화된 관리 체계를 수립하고 운영을 위한 프레임워크 및 저장소를 구축하는 것
- 구성요소: 원칙, 조직, 프로세스
- 체계: 데이터 표준화, 데이터 관리 체계, 데이터 저장소 관리, 표준화 활동

# 4과목
p.34 벡터 Vector
- 벡터들은 동질적: 한 벡터의 모든 원소는 같은 자료형 또는 같은 모드를 가짐
- 벡터는 위치로 인덱스됨
- 벡터는 인덱스를 통해 여러 개의 원소로 구성된 하위 벡터 반환 가능
- 벡터 원소들은 이름을 가질 수 있음

p.60 데이터 마트
- 데이터 웨어하우스와 사용자 사이 중간에 위치
- 하나의 주제 또는 하나의 부서 중심의 데이터 웨어하우스

p.69 변수 중요도
- 변수선택법과 유사한 개념으로 모형을 생성하여 사용된 변수의 중요도를 살피는 과정
- Wilk's Lambda: 집단내 분산 / 총 분산

p.70 변수 구간화
- 연속형 변수를 분석 목적에 맞게 활용하기 위해 구간화를 하여 모델링에 적용
- 일반적으로 10진수 단위로 구간화하지만, 구간을 5개로 나누는 것이 보통이고 7개 이상의 구간을 잘 만들지 않음
- 구간화 방법
  - Binning: 신용평가모형의 개발에서 연속형 변수를 범주형 변수로 구간화하는 데 자주 활용되는 방법 -> 연속형 변수를 오름차수 정렬 후 각각 동일한 개수의 레코드를 50개의 Bin에 나누어 담고 각 깍통의 부실율을 기준으로 병합하면서 최종 5~7개의 Bin으로 부실율의 역전이 생기지 않게 합치면서 구간화
  - 의사결정나무: 세분화 또는 예측에 활용되는 의사결정 나무를 사용하여 입력변수 구간화 -> 동일한 변수를 여러 번의 분리기준으로 사용이 가능하기 때문에 연속 변수가 반복적으로 선택될 경우, 각각의 분리 기준값으로 연속형 변수를 구간화할 수 있음

p.74 결측값 처리 방법
- 단순 대치법: 결측값 존재 레코드 삭제
- 평균 대치법: 데이터 평균으로 대치
  - 비조건부 평균 대치법: 관측 데이터의 평균으로 대치
  - 조건부 평균 대치법: 회귀분석을 활용한 대치법
- 단순 확률 대치법: 평균대치법에서 추정량 표준 오차의 과소 푸정 문제를 보완하고자 고안된 방법 ex. Hot-deck 방법, Nearest Neighborhood 방법
- 다중 대치법: 단순 대치법을 한번만 하지 않고 m번의 대치를 통해 m개의 가상적 완전 자료를 만드는 방법

p.75 이상값 Outlier
- 이상값 인식 방법
  - ESD Extreme Studentized Deviation: 평균으로부처 3표준편차 떨어진 값 = 각 0.15%
  - 기하평균-2.5x표준편차 < data < 기하평균+2.5x표준편차
  - Q1-.15(Q3-Q1) < data < Q3+1.5(Q3-Q1)를 벗어나는 데이터 -> 사분위수 이용(상자 그림의 outer fence 밖에 있는 값)
- 극단값 절단 방법: 극단값 절단 방법으로 데이터를 제거하는 것 보다는 극단값 조정 방법을 이용하는 것이 데이터 손실율도 적고, 설명력도 높아짐
  - 기하평균을 이용한 제거
  - 하단 상단 % 이용한 제거: 10% 절단(상하위 5%에 해당되는 데이터 제거)
- 극단값 조정 방법 Winsorizing: 상한값과 하한값을 벗어나는 값들을 하한, 상한값으로 바꾸어 활용하는 방법

p.89 표본 추출 방법
- 단순랜덤추출법: 각 샘플에 번호를 부여하여 임의의 n개를 추출하는 방법 -> 복원, 비복원 추출
- 계통추출법: 단순랜덤추출법의 변형된 방식, 임의 위치에서 매 k번째 항목 추출 방법
- 집락추출법: 군집을 구분하고 군집별로 단숨랜덤추출법 수행 -> 지역표본추출, 다단계표본추출
- 층화추출법: 유사한 원소들끼리 몇 개의 층으로 나누어 각 층에서 랜덤 추출하는 방법 -> 비례층화추출법, 불비례층화추출법

p.93 확률분포
- 이산형 확률 분포: 이산점에서 0이 아닌 확률 값을 가지는 확률 변수 -> 확률질량함수
  - 베르누이 확률분포: 결과가 2개만 나오는 경우
  - 이항분포: 베르누이 시행을 n번 반복했을 때 k번 성공 확률
  - 기하분포: 성공확률이 p인 베르누이 시행에서 첫번째 성공이 있기 까지 x번 실패할 활률
  - 다항분포: 이항분포 확장, 세 가지 이상의 결과를 가지는 반복 시행에서 발생하는 확률 분포
  - 포아송분포: 시간과 공갠 나에서 발생하는 사건의 발생횟수에 대한 확률 분포
- 연속형 확률 분포: 특정 실수 구간에서 0이 아닌 확률을 갖는 확률 변수 -> 확률밀도함수
  - 균일분포=일양분포: 모든 확률 변수x가 균일한 확률을 가지는 확률분포 ex.다트의 확률 분포
  - 정규분포: 평균이 뮤이고 표준편차가 시그마인 X의 확률밀도함수 -> 표준편차가 클수록 퍼져보이는 그래프
  - 지수분포: 어떤 사건이 발생할 때까지 경과 시간에 대한 연속확률분포
  - t분포: 표준정규분포와 같이 평균이 0을 중심으로 좌우가 동일한 분포를 따름 -> 두 집단의 평균이 동일한지 알고자할 때
  - 카이제곱분포: 모평균과 모분산이 알려지지않은 모집단의 모분산에 대한 가설 검정에 사용되는 분포 -> 두 집단 간의 동질성 검정에 활용됨(범주형 자료에 대해 얻어진 관측값과 기대값의 차이를 보는 적합성 검정에 활용)
  - F분포: 두 집단간 분산의 동일성 검정에 사용됨 -> 자유도가 커질수록 정규분포에 가까워짐
  - 누적분포함수 확률분포와의 관계는 확률변수의 누적분포 함수는 그 확률 분포를 유일하게 결정함

p.106 비모수검정
- 모수적 방법: 검정하고자하는 모집단의 분포에 대한 가정을 하고, 그 가정하에서 검정통계량과 검정통계량의 분포를 유도해 검정을 실시하는 방법
- 비모수적 방법: 자료가 추출된 모집단의 분포에 대한 아무 제약을 가하지 않고 검정을 실시 -> 관측된 자료가 특정 분포를 따른다고 가정할 수 없는 경우, 관측된 자료 수가 많지 않거나(30 미만) 자료가 개체 간의 서열 관계를 나타 내는 경우
  - 부호 검정, 윌콕슨의 순위합 검정, 윌콕슨의 부호 순위검정 등
- 차이점
  - 가설의 검정 
    - 모수적 방법: 가정된 분포의 모수에 대해 가설 설저
    - 비모수적 방법: 가정된 분포가 없으므로 다설은 단지 분포의 형태가 동일하다/동일하지 않다와 같이 분포의 형태에 대해 설정
  - 검정 방법
    - 모수적 방법:관측된 자료를 이용해 표본평균, 표본분산 등을 이용해 검정 실시
    - 비모수적 방법: 관측의 절대적인 크기에 의존하지 않는 관측값들의 순위(rank)나 두 관측값 차이의 부호 등을 이용해 검정

p.110 산포의 측도
- 변동계수 Coefficient of Varaiation, CV: 표준편차를 평균으로 나눈 값, 서로 다른 아이템의 편차를 비교하기 위해 사용
  - 체중의 편차와 신장의 편차를 비교할 때 그대로 비교하는 것은 별 의미가 없음 서로 상이한 종류의 측정이기 때문! 그래서 도입하는 것이 변동계수 -> 각각의 통계에서 표준편차를 평균값으로 나누어 주어서 얻은 값을 서로 비교함으로써 체중과 키 중에 어느 것이 더 평균 가까이에 집중되어 있고 어느 것이 더 넓게 산포되어 있는지 비교할 수 있음

p.111 분포의 형태에 관한 측도
- 왜도: 분포의 비대칭정도를 나타내는 측도
  - 왜도가 양수인 경우: 왼쪽으로 밀집되어 있고 오른쪽으로 긴 꼬리, 최빈값 < 중앙값 < 평균
  - 왜도가 음수인 경우: 오른쪽으로 밀집되어 있고 왼쪽으로 긴 꼬리, 최빈값 > 중앙값 > 평균
  - 왜도가 0인 경우: 좌우대칭, 최빈값 = 중앙값 = 평균
- 첨도: 분포의 중심에서 뾰족한 정도를 나타내는 측도
  - 첨도가 양수인 경우: 표준정규분포보다 더 뾰족
  - 첨도가 음수인 경우: 표준정규분포보다 덜 뾰족
  - 첨도가 0인 경우: 표준정규분포와 유사한 뾰족

p.117 상관분석 유형

| 구분   | 피어슨                                        | 스피어만                     |
|------|----------------------------------------------|------------------------|
 | 개념   | 등간척도 이상으로 측정된 두 변수들의 상관관계 측정 방식 | 서열척도인 두 변수들의 상관관계 측정 방식|
| 특징 | 연속형 변수, 정규성 가정 | 순서형 변수, 비모수적 방법|
| 상관계수 | 피어슨 r (적률상관계수) | 순위상관계수(p, 로우) |

- 스피어만 상관계수
  - 일반적으로 서열상관계수는 집단 내의 개별 관측치를 두 개의 서로 다른 관점이나 특성으로 평가한 순위값들을 이용해서 분석하는 경우에 사용
  - 두 변수의 순위 사이의 의존성을 측정하는 비모수 척도로 단조함수를 통해 두 변수의 관계가 얼마나 잘 설명될 수 있는지를 판단
  - 선형여부와 관계 없이 두 변수가 단조적 관계까 있는지 평가 
    - cf. 피어슨 상관계수: 두 변수 사이의 선형 관계 평가

p.121 t검정
- 두 집단의 평균을 통계쩍으로 비교하기 위해 사용하는 검정 방법
- 일표본 t검정 one sample t-test: 단일 모집단에서 관심이 있는 연속형 변수의 평균 값을 특정 기준 값과 비교하고자 할 때
  - 가정
    - 모집단의 구성요소들이 정규분포를 이룬다
    - 종속변수는 연속형, 검증하고자하는 기준값이 있어야 함
- 대응표본 t검정 paired sample t-test: 단일모집단에 대해 두 번의 처리를 가했을 때, 두 개의 처리에 따른 평균의 차이를 비교하고자 할 때
  - 하나의 모집단에서 크기가 n개인 하나의 표본을 추출한 후, 표본 내의 개체들에 대해서 두 번의 측정을 실시
  - 모집단과 표본은 하나씩이지마, 각 개체들에 대해 두 개씩의 관측값이 존재하므로 모수는 두 개
  - 가정
    - 모집단의 관측값이 정규성(정규분포를 만족한다는 가정)을 만족해야 함
    - 일반적으로 표본의 크기가 충분히 클 때 중심극한정리에 따라 정규성을 만족한다고 봄
    - 종속변수는 연속형
- 독립표본 t-검정 Independent Sample t-test
  - 두 개의 독립된 모집단의 평균을 비교할 때
  - 두 개의 모집단에서 크기가 n개인 표본을 각각 추출한 후 표본의 관측값들을 이용해 검정 실시
  - 모집단, 모수, 표본이 모두 두 개씩 존재
  - 가정
    - 두 모집단은 정규성을 만족해야 함 -> 표본의 크기가 충분히 크다면 중심극한정리에 따라 정규성을 만족한다고 볼 수 있음
    - 두 개의 모집단은 서로 독립적
    - 두 모집단의 분산이 서로 같음을 의미하는 등분산성 가정을 만족해야 함 -> 등분산성 가정을 확인하기 위해 독립표본 t검정에서 검정통계량을 계산하기 전에 등분산 검정을 먼저 수행해야 함
      - 등분산성 가정: 두 독립 집단의 모분산이 동일해야 함
    - 독립변수는 범주형, 종속변수는 연속형

p.130 분산분석 ANOVA
- 두 개 이상의 집단에서 그룹 평균 간 차이를 그룹 내 변동에 비교하여 살펴보는 방법
- 두 개 이상 집단들의 평균 간 차이에 대한 통계적 유의성을 검증하는 방법
- 분류
  - 단일변량 분산분석
    - 일원배치 분산분석: 독립변수 1개, 종속변수 1개
    - 이원배치 분산분석: 독립변수 2개, 종속변수 2개
    - 다원배치 분산분석: 독립변수 3개 이상, 종속변수 1개
  - 다변량 분산분석
    - MANOVA: 독립변수 1개이상, 종속변수 2개 이상 

p. 131 일원배치 분산분석 One-way ANOVA
- 분산분석에서 반응값에 대해 하나의 범주형 변수의 영향을 알아보기 위해 사용되는 검증 방법
- 모집단의 수에는 제한이 없음, 각 표본의 수는 같지 않아도 됨
- F 검정 통계량을 이용
- 가정
  - 각 집단의 측정치는 서로 독립적, 정규분포를 따름(정규성 가정)
  - 각 집단의 측정치의 분산은 같음 (등분산 가정)
- 사후검정
  - 분산분석의 결과 귀무가설이 기각되어도 적어도 한 집단에서 평균의 차이가 있음이 통계적으로 증명되었을 경우, 어떤 집단들에 대해서 평균의 차이가 존재하는지 알아보기 위해 실시하는 분석
  - 종류: 던칸의 MRT, 피셔의 최소유의차 방법, 튜키의 HSD방법, Scheffe 방법 등이 있음
- R에서 주의할 점: 그룹을 구분하는 기분이 되는 변수가 반드시 factor형이어야 함

p.134 이원배치 분산분석 Two-way ANOVA
- 분산분석에서 반응값에 대해 두 개의 범주형 변수 A,B의 영향을 알아보기 위해 사용되는 검증 방법
- 두 독립변수 A,B 사이에 상관관계가 있는지 살펴보는 교호작용(두 독립변수의 범주들의 조합으로 인해 반응변수에 미치는 특별한 영향)에 대한 검증이 반드시 진행되어야 함
- 가정
  - 각 집단 측정치의 분포는 정규분포(정규성)
  - 각 집단의 측정치의 분산은 같음 (등분산 가정)
- 주효과와 교호작용효과
  - 이원배치 분산분석에서는 두 개의 독립변수값에 따르는 데이터의 주효과와 교호작용효과에 대한 검정을 수행
  - 주효과란 각각의 독립변수가 종속변수에 미치는 효과를 의미, 이를 검정하는 것이 주효과 검정
  - 교호작용효과는 여러 독립변수들의 조합이 종속변수에 주는 영향을 의미
  - 즉 교호작용 효과 검정은 한 독립변수가 종속변수에 미치는 영향이 다른 독립변수의 수준에 따라서 달라지는지를 분석
  - 두 독립변수 A,B 사이의 상관관계가 존재할 경우, 교호작용이 있다는 의미
  - 교호작용이 없을 경우, 주효과 검정을 진행
  - 교호작용이 있을 경우, 주효과 검정이 무의미함

p.136 실험계획법 DOE, Design Of Experiment
- 개념: 시스템이나 프로세스의 결과에 영향을 미치는 인자를 도출하고, 측정 데이터를 통계적으로 분석하기 위한 실험을 설계하는 방법, 최소 실험 횟수로 최대 정보를 얻는 것이 목적
- 목적
  - 분산분석 및 검정과 추정의 문제
  - 최적 반응 조건의 결정 문제
  - 오차항 추정의 문제
- 실험 계획의 원리
  - 랜덤화의 원리
  - 반복의 원리
  - 블록화의 원리
  - 직교화의 원리
  - 교락의 원리
- 주요 용어
  - 인자 Factor: 실제 실험 대상, 입력변수 X
  - 특성치 Characteristic Value: 실험의 모든 결과값, 출력변수 Y
  - 수준 Level: 실험하기 위한 인자의 조건, 인자의 정도나 값
  - 주효과 Main Effect: 각 입력 변수의 수준간 차이, 인자가 독립적으로 반응에 미치는 영향 
  - 교호효과 Interaction Effect: 특정한 인자 수준의 조합에서 일어나는 효과, 인자들이 혼합되어 반응에 미치는 영향
  - 교락 Confounding: 2개 이상의 효과(주효과 또는 교호효과)를 구별할 수 없도록 계획적으로 조합하는 것
  - 블록 Block: 실험 단위가 균일할 수 있도록 단위를 모은 것
  - 반복 Replication: 인자들의 동일한 수준 조합에서 다회의 실험을 진행
  - 중복 Repetition: 한 실험에서 여러 개의 대상을 측정
- 종류
  - 요인배치법 Factorial Design: 모든 인자 간의 수준 조합에서 실험이 이루어지는 완전랜덤화방법, 교호효과를 포함한 모든 요인효과를 추정 가능 
  - 분할법 Split-plot Design: 완전랜덤화가 힘들 경우, 몇 단계로 분할하여 각 단계별로 완전 랜덤하게 실험 순서를 결정하는 방법
  - 교락법 Confounding method: 검출할 필요가 없는 교호작용을 다른 요인과 교락하도록 배치하는 방법, 고차의 교호작용을 블록에 교락시키기 때문에 주효과가 높게 추정됨
  - 난괴법 Randomized Block Design: 실험 단위를 몇 개의 반복으로 나누어 배치하는 방법, 실험 오차를 줄일 수 있기 때문에 효율이 높고 비교적 분석이 간단
  
p.138 교차분석(검정)
- 범주형 자료인 두 변수 간의 관계를 알아보기 위해 실시하는 분석기법
- 적합도 검정, 독립성 검정, 동질성 검정에 사용
- 카이제곱 검정 통계량 이용
- 적합도 검정
  - 실험에서 얻어진 관측값들이 예쌍한 이론과 일치하는지 아닌지를 검정하는 방법
  - 모집단 분포에 대한 가정이 옳게 됐는지를 관측 자료와 비교하여 검정하는 것
- 독립성 검정
  - 모집단이 두 개의 변수에 의해 범주화 되었을 때, 이 두 변수들 사이의 관계가 독립인지 아닌지 검정하는 것을 의미
- 동질성 검정
  - 모집단이 임의의 변수에 따라 R개의 속성으로 범주화되었을 때, R개의 부분 모집단에서 추출한 각 표본인 C개의 범주화된 집단의 분포는 서로 동일한지 아닌지를 검정하는 것을 의미

p.142 중심극한정리 Central Limit Theorem
- 모집단의 분포가 어떤 분포를 따르는지에 관계 없이 표본의 개수 n이 커질수록 표본 평균의 분포(표집분포)가 정규분포에 가까워지는 현상
- 표본의 크기가 커질수록 표봉평균은 모집단의 평균에 가까워진다는 의미를 가짐

p.144 회귀분석
- 선형회귀분석 가정
  - 독립변수와 종속변수 간의 선형성: 입력변수와 출력변수의 관계가 선형이어야 함(가장 중요)
  - 오차의 등분산성: 오차의 분산은 독립변수 값과 무관하게 일정해야 함 -> 잔차플롯을 그렸을 때 잔차와 독립변수 간 아무런 관련성이 없게 점들이 무작위적으로 고르게 분포되어야 함
    - 오차: 종속변수의 예측값과 실제 관측값 간의 차이
  - 오차의 정규성: 오차의 분포가 정규분포를 만족해야 함 -> Q-Q plot, Kolmogorov-Smirnov 검정, Shapiro-Wilk 검정 등
  - 오차의 독립성: 오차들은 서로 독립적이라는 가정, 예측값의 변화에 따라 오차항이 특정한 패턴을 가져서는 안됨 -> Durbin Watson 검정 수행하여 통계량이 2에 가까울수록 오차항의 자기상관이 없음을 의미
- 회귀분석 종류에 따른 가정에 대한 검증
  - 단순선형회귀분석: 입력변수와 출력변수 간의 선형성을 점검하기 위해 산점도 확인
  - 다중선형회귀분석: 위 4가지 모두

p.146 단순선형회귀분석
- 회귀계수 추정방법: 최소제곱법
  - 최소제곱법(최소자승법): 잔차 제곱합을 최소로 만드는 직선을 찾는 것
- 결과 해석
  - 회귀모형은 통계적으로 유의한가? = F검정
  - 회귀계수는 통계적으로 유의한가? = t검정
  - 모형은 데이터를 얼마나 설명할 수 있는가? = 결정계수 확인
  - 모형이 데이터를 잘 적합하고 있는가? = 모형의 잔차를 그래프로 그리고, 회귀진단 수행

p.150 다중선형회귀분석
- 분석시 검토사항
  - 데이터가 전제하는 가정을 만족시키는가? = 선형회귀분석 가정 4가지 만족시키는지 확인
  - 다중공선성 확인: 다중공선성이 있다면, 문제가 있는 독립변수를 제거하거나 주성분회귀(PCA), 릿지회귀모형과 같은 다른 추정방법을 이용해야 함
    - 다중공선성: 독립변수들 간에 강한 상관관계가 나타나는 문제
      - 검사 방법독립변수들 간의 상관계쑬르 구하여 상관성을 직접 파악
      - 허용오차를 구했을 때 0.1이하면 다중공선성 문제가 심각
      - 분산팽창요인(VIF)는 허용오차의 역수, 일반적으로 10 이상이면 심각함!
- 결과 해석
  - 회귀모형은 통계적으로 유의한가? = F검정
  - 회귀계수는 통계적으로 유의한가(? = t검정
  - 모형은 데이터를 얼마나 설명할 수 있는가? = 결정계수 확인
  - 모형이 데이터를 잘 적합하고 있는가? = 모형의 잔차를 그래프로 그리고, 회귀진단 수행

p.152 회귀분석 종류
- 단순회귀: 독립변수 1개, 종속변수와의 관계까 직선
- 다중회귀: 독립변수 k개, 종속변수와의 관계가 선형(1차 함수)
- 로지스틱 회귀: 종속변수가 범주형(2진변수)
- 다항회귀: 독립변수와 종속변수와의 관계가 1차함수 이상인 관계
- 곡선회귀: 독립변수가 1개, 종속변수와의 관계가 곡선
- 비선형회귀: 회귀식의 모양이 미지의 모수들의 선형관계로 이뤄져 있지 않은 모형

p.155 최적회귀방정식
- 최적회귀방정식 선택
  - 모형 내 설명변수의 수가 증가할수록 데이터 관리에는 많은 노력 요구
  - 변수 선택시 f-통계량이나 AIC와 같은 특정 기준을 근거로 변수를 제거하거나 선택함
    - F통계량의 유의확률이 유의수준보다 큰 변수는 통계적으로 유의하지 않으므로 제거해야 함
    - AIC와 같은 벌점화 기준을 가장 낮게 만드는 변수 조합을 선택해야 함
  - 단계적 변수선택 Stepwise Variable Selection
    - 전진선택법 forward selection
      - 장점: 이해하기 쉽고, 변수 개수가 많은 경우에도 사용 가능
      - 단점: 변수값의 작은 변동에도 결과가 크게 달라져 안정성 부족, 한번 선택된 변수는 제거할 수 없음
    - 후진 제거법 backward elimination
      - 장점: 전체 변수들에 대한 정보 이용
      - 단점: 변수 개수가 많은 경우 사용 어려움, 한번 선택된 변수는 제거할 수 없음
    - 단계적 방법 stepwise method
      - 전진선택법에 의해 변수를 추가하면서 새롭게 추가된 변수에 의해 기존 변수의 중요도가 약화되면 해당 변수를 제거
      - 변수 추가, 제거를 수행하면서 기준 통계치를 가장 많이 개선시킬 때까지 반복
      - 반대로 후진 제거법과 같이 모든 변수가 포함된 모델에서 시작하여 변수의 제거 또는 추가를 반복
      - 장점: 모든 변수 조합을 고려하는 방법으로 가장 좋은 변수 조합 선택 가능
      - 단점: 변수 개수가 많을수록 검증할 회귀분석이 많아서 계산량이 늘어나고 계산시간이 길어짐
- 벌점화된 선택기준

# 5과목

p.10 사용가능한 데이터 확인
- 데이터 명세화: 차원과 측정값
  - 모든 데이터는 기본적으로 하나 이상의 측정값 Measure과 하나 이상의 차원 Dimension을 갖음
  - 측정값을 분류할 수 있는 모든 것이 차원이 될 수 있음
  - 연속적인 데이터로 구성된 차원은 구간 형태로 재구성되기도 함
  - 동일한 데이터 항목이라도 차원이 될 수 있고 측정값도 될 수 있음
- 데이터 구성 원리1: 이벤트 기록으로서 접근
  - 원본 데이터는 명세화의 기본 대상, 특정 이벤트가 발생했을 때 생성됨 
  - 데이터가 어떤 원리로 생성, 구성되었는지 염두해야 함, 대상들 간의 관계는 시각화 도구를 활용해 찾아낼 수 있음
- 데이터 구성 원리2: 객체지향 관점에서의 접근
  - 데이터 구성과 생성 배경에 대해 고민함으로써 어떻게 시각화할지 찾을 수 있음
  - 데이터의 구조 자체를 설계, 생성하여 이를 토대로 통찰을 찾을 수 있음
  - 기본적으로 대상을 객체화하며, 모든 객체들은 행위와 고유 속성 값을 갖게 됨
  - 구조의 행위를 통해 구조 전체를 파악하는 것이 객체지향 관점

p.16 비정형 데이터에서의 관계 탐색
- 워들 Wordle: 주어진 텍스트 데이터에서 의미를 갖는 형태소 단위를 추출한 뒤 그것들의 빈도를 계싼해 빈도에 따라 색상이나 크기를 결정한 시각화 기법

p.21 지표 설정과 분석
- 지표: 어떤 현상의 강도를 평가하는 기준이 되는 숫자
- 기본 구조
  - 앞서 도출한 관계를 무언가 하나의 지표로 축약해 표현하면 다른 관계를 살펴보기 위한 기준으로 삼기가 훨씬 편해짐
  - 지표는 기존 값들을 어떤 함수식에 적용한 결과
- 주의점
  - 지표 단위
  - 지표가 통계 모델 만들 때 포함되면, 모델 설명력이 과대평가 될 수 있음
  - 요인분석: 지표가 지표를 만든 다른 요인들과 상당 부분 설명력이 겹치는지의 여부 확인

p.23 통찰에 대한 필요성
- 기존 문제 해결방식이나 설명 모델의 수정
- 새로운 문제 해결 방식 도입
- 새롭게 발견한 가능성에 대한 구체적인 탐색과 발전

p.26 지표의 운영
- 관계는 보통 여러 차원과 측정값 사이의 패턴을 말해주기 때문에 지표를 활용한다는 것은 여러가지 관계를 다 살펴보는 부담을 덜어줌
- 몇 가지의 지표만 집중해봐도 다양한 관계들을 통해 나타나는 전체적인 흐름을 알 수 있음
- 인사이트 프로세스에서 추출한 지표를 중심으로 운영할 경우, 문제점 -> 바로 환산된 값을 중심으로 보다보니 정작 어떤 변화요인이 발생해 지표 흐름에 영향을 미쳤는지 찾아내기가 어려워짐
- 지표를 운영할 때는 지표의 장단점을 이해하고 인사이트의 발전과 확장 효율성을 높일 수 있는 방향으로 지표를 끌고갈 필요가 있음

p.43 인포그래픽
- 정보사용의 목적과 관점에 따라 전달하고자 하는 메시지는 두가지 유형으로 나뉨
  - 정보형 메시지
    - 객관적 정보를 전달하는데 목적을 두는 것
    - 실제를 왜곡하더라도 사람들이 보기쉽도록 개념적으로 구현할 수 있음
  - 설득형 메시지
    - 주장하는 바를 알리는 데 목적을 두는 것
    - 정보 자체를 전달하기 보다는 시각적으로 강렬하게 주장하는 바를 전달하기 위해 사용

p.45 빅데이터 시각화 영역
- 메시지 전달 관점에서의 시각화
  - 데이터 시각화: 같은 범주 안의 많은 양의 데이터에 의미를 부여해 효율으로 전달하기 위한 것
  - 정보 시각화: 큰 범주에 해당하는 정보를 시각화하는 것

p.47 정보 디자인 프로세스 10단계
- 데이터 수집 -> 모든 것을 읽기 -> 내러티브 찾기 -> 문제 정의 -> 계층 구조 만들기 -> 와이어프레임 그리기 -> 포맷 선택 -> 시각 접근 방법 결정 -> 정제와 테스트 -> 세상에 선보이기
- 내러티브 Narrative: 스토리텔링과 유사한 의미, 실화나 허구의 사건들을 묘사하는 것, 그 자체뿐만 아니라 이야기를 조직하고 전개하기 위해 이용되는 각종 전략이나 형식 등을 포괄하는 개념

p.57 시각화 방법의 개념
- 정보 구조화: 데이터 수집 및 탐색, 분류, 배열, 재배열
  - 데이터 멍잉 Data Munging: 정보 조직화에 해당하는 과정, 원 데이터의 구문을 분석, 정리하고, 집단으로 묶거나 변환해 패턴을 식별하거나 특정 정보를 추출하는 과정
  - 정보 조직화를 위한 LATCH 방법: 위치 LATCH, 알파벳 ALPHABET, 시간 TIME, 카테고리 CATEGORY, 위계 HIERARCHY 5가지가 정보를 정리 혹은 조직화하는 기준
    - 위치: 정보를 공간적인 위치에 배열하는 방법
    - 알파벳: 사전, 전화번호부와 같이 방대한 정보를 조직화할 때 알파벳 또는 가나다순으로 정렬하는 방법
    - 시간: 연도별 시간 순서
    - 카테고리: 정보의 속성에 따라 분류
    - 가중치(위계): 물량의 변화(적음 -> 많음), 질량의 변화(가벼움 -> 무거움), 고도의 변화(낮음 -> 높음), 가격의 변화(싼 것 -> 비싼 것) 등 정보의 변화에 따라 데이터 값이나 중요도의 순서로 정보를 조직화하는 것
    - 관계 맺기(재배열): 데이터에 의미를 부여하는 가장 기본적인 과정 
- 정보 시각화: 시간, 분포, 관계, 비교, 공간 시각화, 여러 변수 비교
- 정보 시각표현: 그래픽 7요소, 그래픽 디자인 기본 원리, 인터랙션, 시각정보 디자인 7원칙

p.64 시간시각화
- 누적 막대그래프: 전체의 합이 의미가 있는 경우에 사용
- 점그래프: 한 점에서 다음 점으로 변하는 점의 집중 정도와 배치에 따라 흐름을 파악하기 용이

p.66 분포시각화
- 일반적인 특성은 최대, 최소, 전체분포로 나뉨
- 분포 그래픽에서 가장 주목해야 할 것은 분포정도
- 분포 데이터는 부분을 전부 합치면 1 또는 100%가 됨
- 데이터 특성에 맞도록 전체의 관점에서 부분 간의 관계를 보여줘야 함
- 원그래프: 면적을 값을 보여주고, 수치를 각도로 표시
- 도넛차트: 조각에 해당하는 수치를 면적이 아닌 길이로 표시(중심 구멍 때문)
- 트리맵: 영역기반 시각화, 단순 분류 별 분포 시각화에도 쓸 수 있지만, 위계 구조가 있는 데이터나 트리 구조의 데이터를 표시할 때 활용 가능

p.70 관계 시각화
- 히스토그램:도수분포표의 각 계급을 가로축에 나타내고 해당 계급에 속하는 측정값의 도수를 세로축에 표시하여 직사각형 모양으로 그려놓은 그래프

p.72 비교 시각화
- 히트맵: 여러가지 변수를 비교할 수 있고, 한 칸의 색상으로 데이터 값을 표현, 데이터가 지나치게 많을 경우 더 혼란스러울 수 있으니, 적당한 색상을 선택하고 약간의 정렬 과정을 거쳐야 함
- 체르노프 페이스: 데이터를 사람의 얼굴 이미지로 표현하는 방법, 통상 유용성 보다는 전문가의 흥미가 주목적
- 스타차트(거미줄차트, 방사형차트): 중심점은 축이 나타내는 값의 최솟값, 가장 먼 끝 점은 최댓값
- 평행 좌표계: 여러 축을 평행으로 배치해서 만들고 y축에서 윗부분을 변수 값 범위의 최댓값, 아래는 변수값 범위의 최솟값을 나타냄
- 다차원척도법: 데이터 세트상의 개별 데이터 간의 유사도를 바탕으로 시각화, 대상 간의 유사성 측도에 의거해 대상을 다차원 공간 속에 배치시키는 방법 -> 표현하고자하는 객체 간 간격이 발생하는, 거리행렬을 포함하는 데이터 시각화에 유용

p.79 자크 베르탱 그래픽 7요소
- 위치
- 크기: 크기가 크고 작은것이 아니라 주의 크기와 같은지 다른지 여부가 중요
- 모양: 비슷한 형태가 아닌 전혀 다른 형태로 바꾸는 것이 중요
- 색
- 명도: 같은 색의 요소들 중 하나만 명도가 유난히 높거나 낮다면 강조됨
- 기울기: 같은 요소들 중 하나만 기울기가 다르다면 강조될 수 있음
- 질감: 질감을 지나치게 많이 쓰면 좋지 않은 결과를 가져올 수 있으므로 신중한 선택 필요

p.82 시각화를 위한 그래픽 디자인 기본 원리
- 타이포그래피
- 색상 
- 그리드
  - 개념: 그리드를 이용해 블록 레이아웃을 잡고 그 위에 요소를 효율적으로 올려놓아 전체적인 조화 추구
  - 하나의 화면을 읽는 방식: 인간의 눈은 습관적으로 왼쪽 상단부터 오른쪽 하단 귀퉁이로 훑어 내려감
  - 정보의 역피라미드
    - 맨 위: 가장 중요하고 강력한 정보가 
    - 그 다음: 2차 정보
    - 마지막: 일반적인 정보
- 아이소타이프
  - 많은 양의 데이터를 쉽게 지각할 수 있도록 도와주는 시각표현 방법
  - 정보, 자료, 등을 나타내기 위해 문자와 숫자 대신 상징적인 도형이나 정해진 기호를 조합해 시각적이고 직접적으로 나타내는 방식
  - 단순히 우리 눈에 익숙한 픽토그램을 뜻하는 것이 아니라 하나의 기호가 일정한 수량을 대표

p.95 시각 정보 디자인 7원칙
1. 시각적 비교 강화
2. 인과관계 제시
3. 다중변수 표시
4. 텍스트, 그래픽, 데이터를 한 화면에 조화롭게 배치
5. 콘텐츠의 질, 연관성, 진실성을 분명히
6. 시간순이 아닌 공간순 나열
7. 정량적 자료의 정량성 제거하지 않음

p.157 모자이크 플룻
- 복수의 카테고리컬 변수 분포 파악에 도움이 되는 시각화 방법
- 두 변수의 구조적 특징 파악 가능, 핵심내용을 간단하게 전달 가능

p.179 D3.js
- 자바스크립트 기반 데이터 시각화 라이브러리
- html5, svg, css로 데이터 시각화를 하고 SVG객체, canvas 객체 등을 기반으로 동작
- D3.js의 모든 시각화 요소들은 html문서의 svg 객체로 표현되며, 자바스크립트를 통해 이 객체를 생성, 조작할 수 있음
- css를 통해 객체의 레이아웃과 속성을 변경해 디자인적 요소를 조작 가능
- 파이어폭스, 크롬, 사파리, 오페라, IE9을 사용하는 한 동일한 코드에서 일관적인 결과를 얻을 수 있음

p.180 D3.js 데이터 표현
- 가장 중요한 거은 매핑의 스케일 scale
- D3.js에서는 시각적 요소에 데이터를 직접 입력하는 대신, scale이라는 객체로 데이터와 시각적 요소 간의 관계를 정의
- scale로 관계를 정의하는 것은 고정적인 시각화 구현에서는 필요하지 않더라도, 한 번의 구현으로 다양한 화면의 크기에서 동작해야 하는 시각화에서 매우 유용함
- scale 객체를 통해 다양한 화면 해상도에서 깨짐 없는 시각화 구현 가능

p.180 D3.js 시각화 구현
- SVG 객체: 그림을 그리기 위한 HTML 태그, D3를 활용해 시각화를 구현하기 위해서는 HTML5의 SVG객체가 필요
- Scale 함수
  - 각기 다른 데이터 정보에 따라 SVG로 구현한 시각화 그림들이 화면에 출력되는 과정에서 다소 부자연스럽게 표현되는 것을 방지하기 위해 사용하는 것
  - 시각화를 구현하고자 하는 공간에 그 크기와 컬러들을 자동으로 조절하여 시각화의 최적화 지원
  - 스케일링한다는 것은 읽어온 데이터 값을 건드리지 않고, 각 데이터 값에 맞는 크기와 컬러 범위를 출력장치에 맞도록 시각화하는 것을 의미
  - domain(): scale 입력 값의 범위 지정
  - range(): scale 출력 값의 범위 지정
  - .scale(): domain과 range로 설정된 range을 통해 원하는 위치에 무언가를 놓는 것

p.196 Canvas 객체
- 히트맵으로 구현하기 위해서는 Canvas 객체 필요
- 객체에 정보 저장 X
- 다시 그리기 불리함
- 성능 높음

cf. SVG 객체
- 객체에 정보 저장 O
- 다시 그리기 유리함
- 성능 낮음