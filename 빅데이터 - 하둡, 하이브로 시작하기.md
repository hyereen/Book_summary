# 빅데이터 - 하둡, 하이브로 시작하기
- https://wikidocs.net/book/2203

## 2-빅데이터 처리단계
### 정제
- Identification: 알려진 다양한 데이터 포맷이나 비정형 데이터에 할당된 기본 포맷을 식별
- Filtration: 수집된 정보에서 정확하지 않은 데이터는 제외
- Validation: 데이터 유효성을 검증
- Noise Reduction: 오류 데이터를 제거, 분석 불가능한 데이터는 제외
- Transformation: 데이터를 분석 가능한 형태로 변환
- Compression: 저장장치 효율성을 위해 변환한 데이터를 압축
- Integration: 처리 완료한 데이터를 적재

## 3-빅데이터 에코시스템
### 데이터 직렬화
- 빅데이터 에코시스템이 다양한 기술과 언어로 구현되기 때문에 각 언어간에 내부 객체를 공유해야 하는 경우가 있음
- 이를 효율적으로 처리하기 위해 데이터 직렬화 기술 이용

### 에이브로 Avro
- 아파치 하둡 프로젝트에서 개발된 원격 프로시저 호출(RPC) 및 데이터 직렬화 프레임워크
  - 원격 프로시저 호출(영어: remote procedure call, 리모트 프로시저 콜, RPC)은 별도의 원격 제어를 위한 코딩 없이 다른 주소 공간에서 함수나 프로시저를 실행할 수 있게하는 프로세스 간 통신 기술
- 자료형과 프로토콜 정의를 위해 JSON을 사용
- 콤팩트 바이너리 포맷으로 데이터 직렬화

### 스리프트 Thrift
- 페이스북에서 개발한 서로 다른 언어로 개발된 모듈의 통합을 지원하는 RPC 프레임워크
- 데이터 타입과 서비스 인터페이스를 선언하면, RPC 형태의 클라이언트와 서버 코드를 자동으로 생성해줌
- 자바, C++, C#, Perl, PHP, 파이썬 등 다양한 언어 지원

### 프로토콜 버터 Protocol Buffers
- 구글에서 개발한 RPC 프레임워크 
- 구조화된 데이터를 직렬화하는 방식을 제공
- 직렬화 속도가 빠르고 직렬화된 파일의 크기도 작아서 Apache Avro 파일 포맷과 함께 많이 사용됨

### 저장
### 하둡 분산 파일 시스템 HDFS, Hadoop distributed file system
- 하둡 프레임워크를 위해 자바 언어로 작성된 분산 확장 파일 시스템
- 범용 컴퓨터를 클러스터로 구성
- 대용량의 파일을 블록단위로 분할하여 여러서버에 복제하여 저장

### HBase
- DFS 기반의 칼럼 기반 NoSQL 데이터베이스
- 실시간 랜덤 조회 및 업데이트가 가능
- 각 프로세스는 개인의 데이터를 비동기적으로 업데이트 가능
- 기본 동작단위 - 칼럼
- H마스터가 H리전을 관리하하는 구조
- 주키퍼거 H마스터를 관리하여 SPOF를 회피
  - SPOF: 단일 장애 지점, 동작하지 않으면 전체 시스템이 중단되는 요소

### 데이터 처리 
### 맵리듀스 MapReduce
- HDFS 상에서 동작하는 가장 기본적인 분석 기술
- 간단한 단위 작업을 반복할 때 효율적인 맵리듀스 모델을 사용히여 데이터 분석

### 스파크 Spark
- 인메모리 기반의 범용 데이터 처리 플랫폼
- ㄴ 인메모리: 인-메모리 컴퓨팅은 메모리 내에서 데이터의 저장 뿐 아니라 데이터의 연산까지 수행하는 최첨단 칩 기술,메모리 내 대량의 정보를 이동 없이 메모리 내에서 병렬 연산하기 때문에 전력 소모가 낮음

### 임팔라 Impala
- 하둡 기반 분산 쿼리 엔진
- 맵리듀스를 사용하지 않고, C++로 개발한 인메모리 엔진을 사용해 빠른 성능을 보여줌
- 데이터 조회를 위한 인터페이스로 HiveQL 사용

### 프레스토 Presto
- 대화형 질의를 처리하기 위한 분산 쿼리 엔진
- 메모리 기반으로 데이터 처리
- 다양한 데이터 저장소에 저장된 데이터를 SQL로 처리 가능

### 하이브 Hive
- 하둡 기반의 데이터웨어하우징용 솔루션
- SQL과 애무 유사한 HiveQL 사용
- ㄴ HiveQL은 내부적으로 맵리듀스 잡으로 변환되어 실행됨

### HCatalog
- Pig, MapReduce, Spark에서 Hive 메타스토어 테이블에 액세스할 수 있는 도구
- 테이블을 생겅하거나 기타 작업을 수행할 수 있는 REST 인터페이스 및 커맨드라인 클라이언트 제공

### 피그 Pig
- 복잡한 맵리듀스 프로그래밍을 대체할 피그 라틴(Pig Latin)이라는 자체 언어를 제공
- 맵리듀스 API를 매우 단순화한 형태
- SQL과 유사한 형태로 설계되었음
- 다만 기존 SQL 지식을 활용하기 어려움

### 클러스터 관리
### 얀 YARN
- 데이터 처리 작업을 실행하기 위한 클러스터 자원(CPU, 메모리, 디스크 등)과 스케쥴링을 위한 프레임워크
- 기존 하둡의 맵리듀스의 단점을 극복하기 위해 시작됨
- 맵리듀스, 하이브, 임팔라, 타조, 스파크 등 다양한 애플리케이션들은 얀에서 리소스를 할당받아서 작업을 실행하게 됨

### 메소스 Mesos
- 클라우드 인프라 및 컴퓨팅 엔진의 다양한 자원(CPU, 메모리, 디스크)을 통합적으로 관리할 수 있도록 만든 자원 관리 프로젝트
- 클러스터링 환경에서 동적으로 자원을 할당하고 격리해주는 메커니즘 제공 -> 분산 환경에서 작업 실행을 최적화할 수 있음
- 1만대 이상의 노드에도 대응 가능

### 분산 서버 관리
- 클러스터에서 여러가지 기술이 이용될 때 하나의 서버에서 모든 작업이 진행되면 이 서버가 SPOF가 됨
- 이로 인한 리스크를 줄이기 위해 분산 서버 관리 기술 이용

### 주키퍼 Zookeeper
- 분산 환경에서 서버 간의 상호 조정이 필요한 다양한 서비스를 제공하는 시스템
- 역할
  - 하나의 서버에만 서비스가 집중되지 않게 서비스를 알맞게 분산해 동시에 처리하게 해줌
  - 하나의 서버에서 처리한 결과를 다른 서버와도 동기화해서 데이터의 안정성 보장
  - 운영 서버에 문제가 발생해서 서비스를 제공할 수 없는 경우, 다른 대기 중인 서버를 운영 서버로 바꿔서 서비스가 중지 없이 제공되게 함
  - 분산 환경을 구성하는 서버의 환경설정을 통합적으로 관리

### 시각화
### 휴 Hue
- 하둡과 하둡에코시스템의 지원을 위한 웹 인터페이스를 제공하는 오픈 소스
- Hive 쿼릴르 실행하는 인터페이스 제공
- 잡의 스케줄링, 잡, HDFS 등 모니터링하기 위한 인터페이스도 제공

### 보안
### 레인저 Ranger
- 하둡 클러스터의 각 모듈에 대한 보안 정책을 관리할 수 있음
- HDFS의 ACL, Hive 데이터베이스의 접근권한 등의 보안 정책과 각 모듈에 대한 접근 기록(Audit)을 보관함

### 데이터 거버넌스
- 기업의 여기저기 산재한 데이터를 같은 저장소에 관리
- 비정형 데이터를 규칙에 맞게 표준화하는 전사 차원의 빅데이터 관리 체계

### 아틀라스 Atlas
- 데이터 거버넌스로 조직이 보안/컴플라이언스 요구사항을 준수할 수 있도록 지원
- 데이터 자원에 대한 태깅, 다운스크림 데이터셋에 대한 태그전파, 메타 데이터 접그넹 대한 보안 등 다양한 기능을 가지고 있음
- 메타데이터 변경 알림 기능을 제공
- Hive, HBase, Kafka의 데이터가 변경되는 것을 알리는 기능 제공

### 아문센 Amundsen
- 데이터 디스커버리 플랫폼
- 기업에 존재하는 데이터를 검색하고 추천하는 기능을 가지고 있음
- 테이블 상세 페이지 지원

### 플룸 Flume
- 대량의 로그 데이터를 여러 소스에서 수집하여 저장하기 위한 목적으로 개발
- 데이터 수집 프레임워크
- 특징
  - 여러개의 플룸 에이전트를 연결하여 확장 가능
  - 다양한 연결 모드를 지원하여 최종 목적지에 데이터를 전달할 때 까지 유연한 구성 가능
  - 전달 받은 데이터를 메모리, File, DB에 임시 저장하여 오류 발생 시 복구 가능 -> 신뢰성 
- 구조
  - OG: master가 agent를 관리, 데이터가 집중되면 master의 병목현상 발생
    - Agent와 Collector의 설정 값을 변경해줄 때, Master도 함꼐 변경해줘야 하는 불편함이 있음 -> NG 개발
  - NG: MAster와 collector의 개념이 없고 OG의 agent 노드 구성에 해당 

### 카프카 Kafka
- 분산 스트리밍 플랫폼
- 메시징, 메트릭 수집, 로그 수집, 스트림 처리 등 다양한 용도로 사용 가능
- 특징
  - 빠름
  - 확장 가능
  - 안정적
- 발행/구독 모델
  - 주제에 맞게 브로커에게 전달하면 구독자가 브로커에 요청해서 가져가는 방식
  - 발행자는 메시지를 topic으로 카테고리화
  - 구독자는 topic에 맞는 메시지를 브로커에게 요청
  - 발행자와 구독자는 서로 알지 못함
- 발행자 Producer
  - 메세지를 생산하는 주체
  - 메세지를 만들고 브로커에게 토픽으로 분류된 메시지를 전달
    - 메시지는 배치 형태로 전달
  - 발행자는 구독자의 존재를 알지 못함
- 구독자 Consumer
  - 소비자로 메시지를 소비하는 주체
  - 발행자의 존재를 알지 못함
  - 원하는 토픽을 구독하여 스스로 조절해가면서 소비할 수 있음
  - 원하는 토픽의 각 파티션에 존재하는 오프셋의 위치를 기억하고 관리하여 데이터의 중복을 관리

### 스쿱 Sqoop
- 관계형 데이터베이스와 하둡 HDFS간에 데이터를 전송할 수 있도록 설계된 오픈소스 소프트웨어
- 버전
  - Sqoop 1: 클라이언트 방식, CLI 명령여로 작업 실행
  - Sqoop 2: 클라이언트 방식+ 서버사이드 방식 추가됨, Sqoop 서버가 존재하고, 사용자가 서버에 요청하여 작업을 실해앟는 방식
- HDFS와 RDB 간 데이터 전송
  - 양쪽으로 이동 가능
  - Hive, Pig, HBase로 이동 가능
- 동작
  - import: DB -> HDFS
  - export: HDFS -> DB

### 하둡 Hadoop
- 하나의 성능 좋은 컴퓨터를 이용하여 데이터를 처리하는 대신, 적당한 성능의 범용 컴퓨터 여러 대를 클러스터화하고, 큰 크기의 데이터를 클러스터에서 병렬로 동시에 처리하여 처리속도를 높이는 것이 목적
- 분산처리를 위한 오픈소스 프레임워크
- 구성 요소
  - Hadoop Common: 하둡의 다른 모듈을 지원하기 위한 공통 컴포넌트 모음
  - Hadoop HDFS: 분산 저장을 처리하기 위한 모듈, 여러 개의 서버를 하나의 서버처럼 묶어서 데이터 저장
  - Haddop YARN: 병렬처리를 위한 클러스터 자원관리 및 스케줄링 담당
  - Hadoop Mapreduce: 저장된 데이터를 병렬처리할 수 있게 해주는 분산처리 모듈
  - Hadoop Ozone: 하둡을 위한 오브젝트 저장소
- 장점
  - 오픈소스로 라이선스에 대한 비용 부담이 적음 시스템을 중단하지 않고, 장비의 추가가 용이(Scale Out)
  - 일부 장비에 장애가 발생하더라도 전체 시스템 사용성에 영향이 적음(Fault tolerance)
  - 저렴한 구축 비용과 비용대비 빠른 데이터 처리 
  - 오프라인 배치 프로세싱에 최적화
- 단점
  - HDFS에 저장된 데이터를 변경 불가
  - 실시간 데이터 분석 같이 신속하게 처리해야 하는 작업에는 부적합
  - 너무 많은 버전과 부실한 서포트
  - 설정의 어려움

### YARN 아키텍처
- 작업 처리 단위: 컨테이너
- 작업에 제출 -> 애플리케이션 마스터 생성 -> 애플리케이션 마스터가 리소스 매니저에 자원 요청 -> 실제 작업을 담당하는 컨테이너를 할당받아 작업 처리
- 컨테이너는 작업이 요청되면 생성됨 -> 작업이 완료되면 종료되기 때문에 클러스터 효율적 사용 가능
- MR로 구현된 작업이 아니더라도 컨테이너를 할당받아서 동작할 수 있음 -> Spark, HBase, Storm 등 다양한 컴포넌트들 실행 가능

### HDFS Hadoop Distributed File System
- 범용 하드웨어에서 동작
- 장애 복구성을 가지는 분산 파일 시스템 목표
- 실시간 처리보다는 배치 처리를 위해 설계 -> 빠른 데이터 응답시간이 필요한 작업에는 적합 x
- 네임 노드가 단일 실패 지점이 되기 때문에 네임노드 관리 중요
- 특징
  - 블록 단위 저장
    - 블록사이즈보다 작은 파일은 기존 파일 사이즈대로 저장
    - 블록 사이즈보다 크면 블록 단위로 나누어 저장 -> 단일 디스크의 데이터보다 큰 파일도 저장 가능
  - 블록 복제를 이용한 장애 복구
    - 장애 복구를 위해 각 블록을 복제하여 저장
    - 블록의 기본 복제단위는 3 -> 하나의 블록은 3개의 블록으로 복제됨
    - 같은 랙(Rack)의 서버와 다른 랙의 서버로 복제되어 저장됨
    - 블록에 문제가 생기면 복제한 다른 블록을 이용해 데이터 복구
    - 1G 데이터 저장을 위해 데이터가 복제되어 3G 저장공간 필요
  - 읽기 중심
    - 데이터를 한 번 쓰면 여러번 읽는 것을 목적으로 함
    - 파일 수정 지원 x -> 동작 단순화 -> 데이터 읽기 속도 향상
  - 데이터 지역성
    - 맵리듀스는 HDFS의 데이터 지역성을 이용해 처리 속도 증가시킴
    - 처리 알고리즘이 있는 곳에 데이터를 이동시키지 않고, 데이터가 있는 곳에서 알고리즘을 처리 -> 네트워크를 통해 대용량 데이터를 이동시키는 비용을 줄일 수 있음
- 구조
  - 마스터 슬래이브 구조: 하나의 네임노드와 여러 개의 데이터 노드로 구성
    - 네임노드: 메타 데이터 가짐
    - 데이터는 블록 단위로 나누어 데이터 노드에 저장
    - 사용자는 네임노드를 이용해 데이터를 쓰고 읽을 수 있음
  - 네임노드: 메타 데이터 관리와 데이터 노드의 관리
    - 메타 데이터 관리: 각 데이터 노드에서 전달하는 메타 데이터를 받아서 전체 노드의 메타데이터 정보의 파일 정보를 묶어서 관리
    - 메타 데이터는 사용자가 설정한 위치에 저장됨
    - 네임노드가 실행될 때 파일을 읽어서 메모리에 보관
    - 운영 중에 발생한 수정사항은 네임 노드의 메모리에 바로 적용됨
    - 데이터의 수정사항을 다음 구동시 적용을 위해서 주기적으로 Edits 파일로 저장
    - 메타데이터 파일 종류
      - Fsimage 파일: 네임 스페이스와 블록 정보
      - Edits 파일: 파일의 생성, 삭제에 대한 트랜잭션 로그, 메모리에 저장하다가 주기적으로 생성
    - 데이터 노드 관리: 네임노드는 데이터노드가 주기적으로 전달하는 하트비트와 블록리포트를 이용하여 데이터 노드의 동작상태, 블록상태 관리
      - 하트비트: 3초, dfs.heartbeat.interval
        - 하트비트를 이용하여 데이터 노드가 동작 중이라는 것을 네임노드가 알 수 있음
        - 하트비트가 도착하지 않으면 네임노드는 데이터노트가 동작하지 않는 것으로 간주 -> 더이상 IO가 발생하지 않도록 조치
      - 블록리포트: 6시간, dfs.blockreport.intervalMsec
        - 블록 리포트를 이용하여 HDFS에 저장된 파일에 대한 최신 정보 유지
        - 블록리포트에는 데이터 노드에 저장된 블록 목록과 각 블록이 로컬디스크의 어디에 저장되어 있는지에 대한 정보를 가짐
        - 네임노드의 메타데이터 갱신
  - 데이터노드: 파일을 저장하는 역할
    - 파일은 블록단위로 저장됨
    - 주기적으로 네임노드에게 하트비트와 블록리포트 전달
    - 블록파일 저장 형태
      - 블록은 블록과 블록의 메타 정보로 저장됨
      - blk_12345: 파일블록, 블록 복제 개수에 따라 동일한 이름의 블록이 여러 개의 노드에 생성됨
      - blk_12345_29082353.meta: 블록의 메타정보
    - 데이타노드 상태
      - 활성 상태
        - 데이터 노드가 Live상태인지, Dead상태인지 나타냄
        - 데이터노드가 하트비트를 주기적으로 전달하여 살아있는지 확인되면 Live 상태
        - 데이터노드에 문제가 발생하여 지정한 시간동안 하트비트를 받지 못하면 네임노드는 데이터 노드의 상태를 Stale상태로 변경
        - 이후 지정한 시간동안 응답이 없으면 Dead 노드로 변경
      - 운영 상태
        - 데이터 노드의 업그레이드, 패치 작업을 하기 위해 서비스를 잠시 멈추어야 할 경우 블록을 안전하게 보관하기 위해 설정
  - 파일 읽기/쓰기
    - 파일 읽기
      - 네임 노드에 파일이 보관된 블록 위치 요청
      - 네임노드가 블록 위치 반환
      - 각 데이터 노드에 파일 블록 요청
    - 파일 쓰기
      - 네임 노드에 파일 정보 전송하고, 파일의 블록에 써야할 노드 목록 요청
      - 네임노드가 파일을 저장할 목록 반환
      - 데이터 노드에 파일 쓰기 요청
- HDFS Federation
  - 네임노드는 파일 정보 메타데이터를 관리, 파일이 많아지면 메모리 사용량도 늘어남 -> 메모리 관리 문제를 해결하기 위해 하둡v2부터 HDFS 페더레이션 지원
  - 디렉토리(네임스페이스) 단위로 네임노드를 등록하여 사용하는 것
  - 파일, 디렉토리의 정보를 가지는 네임스페이스와 블록의 정보를 가지는 블록 풀을 각 네임노드가 독립적으로 관리
  - 네임스페이스와 블록 풀(두개 합쳐서 네임스페이스 볼륨이라고 함)을 각 네임노드가 독립적으로 관리 -> 하나의 네임노드에 문제가 생겨도 다른 네임노드에 영향 X
- HDFS 고가용성
  - HDFS는 단일 실패 지점 -> 문제 발생 시 모든 작업 중지, 파일 읽고 쓸 수 없게 됨
  - ㄴ 하둡v2에서 이 문제를 해결하기 위해 HDFS 고가용성 제공
  - 이중화된 두 대의 서버인 액티브 네임노드와 스탠바이 네임노드를 이용하여 지원
  - 액티브, 스탠바이는 데이터 노드로부터 블록 리포트와 하트비트를 모두 받아서 동일한 메타데이터 유지, 공유 스토리지를 이용하여 에디트파일 공유
  - 액티브 네임노드는 네임노드의 역할 수행
  - 스탠바이 네임노드는 액티브와 동일한 메타데이터 정보를 유지하다가 액티브에 문제가 발생하면 스탠바이가 액티브 역할을 함
  - ㄴ 액티브 네임노드에 문제가 발생한 것을 자동으로 확인하기 어렵기 때문에 보통 주피퍼를 이용하여 장애 발생시 자동으로 변경될 수 있도록 함
- HDFS 세이프모드
  - 데이터노드를 수정할 수 없는 상태
  - 세이프 모드가 되면 데이터는 읽기 전용 상태가 됨, 데이터 추가와 수정 불가능, 복제도 일어나지 않음
  - 사용 시기: 관리자가 서버 운영 정비(수동), 네임노드에 문제가 생겨서 정상적인 동작을 할 수 없을 때(자동)
- 밸런서 balance
  - 데이터 불균형이 발생하면 실행
  - 데이터 불균형이 발생하는 경우
    - 데이터 노들르 추가하는 경우
    - 대량의 데이터를 삭제하는 경우 -> 데이터 노드간 저장공간 차이가 20~30% 차이나는 경우
    - 대량의 데이터를 추가하는 경우 -> 특정 데이터 노드에 데이터가 적은 경우, 네임노드는 데이터 저장공간이 작은 노드를 우선적으로 사용하는데 이 경우 특정 노드로 I/O가 집중됨
  - 밸런서는 작업간 많은 데이터 이동이 발생하기 때문에 대역폭을 지정하여 다른 작업에 영향이 가지 않도록 권장