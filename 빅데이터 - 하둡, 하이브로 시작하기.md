# 빅데이터 - 하둡, 하이브로 시작하기
- https://wikidocs.net/book/2203

## 2-빅데이터 처리단계
### 정제
- Identification: 알려진 다양한 데이터 포맷이나 비정형 데이터에 할당된 기본 포맷을 식별
- Filtration: 수집된 정보에서 정확하지 않은 데이터는 제외
- Validation: 데이터 유효성을 검증
- Noise Reduction: 오류 데이터를 제거, 분석 불가능한 데이터는 제외
- Transformation: 데이터를 분석 가능한 형태로 변환
- Compression: 저장장치 효율성을 위해 변환한 데이터를 압축
- Integration: 처리 완료한 데이터를 적재

## 3-빅데이터 에코시스템
### 데이터 직렬화
- 빅데이터 에코시스템이 다양한 기술과 언어로 구현되기 때문에 각 언어간에 내부 객체를 공유해야 하는 경우가 있음
- 이를 효율적으로 처리하기 위해 데이터 직렬화 기술 이용

### 에이브로 Avro
- 아파치 하둡 프로젝트에서 개발된 원격 프로시저 호출(RPC) 및 데이터 직렬화 프레임워크
  - 원격 프로시저 호출(영어: remote procedure call, 리모트 프로시저 콜, RPC)은 별도의 원격 제어를 위한 코딩 없이 다른 주소 공간에서 함수나 프로시저를 실행할 수 있게하는 프로세스 간 통신 기술
- 자료형과 프로토콜 정의를 위해 JSON을 사용
- 콤팩트 바이너리 포맷으로 데이터 직렬화

### 스리프트 Thrift
- 페이스북에서 개발한 서로 다른 언어로 개발된 모듈의 통합을 지원하는 RPC 프레임워크
- 데이터 타입과 서비스 인터페이스를 선언하면, RPC 형태의 클라이언트와 서버 코드를 자동으로 생성해줌
- 자바, C++, C#, Perl, PHP, 파이썬 등 다양한 언어 지원

### 프로토콜 버터 Protocol Buffers
- 구글에서 개발한 RPC 프레임워크 
- 구조화된 데이터를 직렬화하는 방식을 제공
- 직렬화 속도가 빠르고 직렬화된 파일의 크기도 작아서 Apache Avro 파일 포맷과 함께 많이 사용됨

### 저장
### 하둡 분산 파일 시스템 HDFS, Hadoop distributed file system
- 하둡 프레임워크를 위해 자바 언어로 작성된 분산 확장 파일 시스템
- 범용 컴퓨터를 클러스터로 구성
- 대용량의 파일을 블록단위로 분할하여 여러서버에 복제하여 저장

### HBase
- DFS 기반의 칼럼 기반 NoSQL 데이터베이스
- 실시간 랜덤 조회 및 업데이트가 가능
- 각 프로세스는 개인의 데이터를 비동기적으로 업데이트 가능
- 기본 동작단위 - 칼럼
- H마스터가 H리전을 관리하하는 구조
- 주키퍼거 H마스터를 관리하여 SPOF를 회피
  - SPOF: 단일 장애 지점, 동작하지 않으면 전체 시스템이 중단되는 요소

### 데이터 처리 
### 맵리듀스 MapReduce
- HDFS 상에서 동작하는 가장 기본적인 분석 기술
- 간단한 단위 작업을 반복할 때 효율적인 맵리듀스 모델을 사용히여 데이터 분석

### 스파크 Spark
- 인메모리 기반의 범용 데이터 처리 플랫폼
- ㄴ 인메모리: 인-메모리 컴퓨팅은 메모리 내에서 데이터의 저장 뿐 아니라 데이터의 연산까지 수행하는 최첨단 칩 기술,메모리 내 대량의 정보를 이동 없이 메모리 내에서 병렬 연산하기 때문에 전력 소모가 낮음

### 임팔라 Impala
- 하둡 기반 분산 쿼리 엔진
- 맵리듀스를 사용하지 않고, C++로 개발한 인메모리 엔진을 사용해 빠른 성능을 보여줌
- 데이터 조회를 위한 인터페이스로 HiveQL 사용

### 프레스토 Presto
- 대화형 질의를 처리하기 위한 분산 쿼리 엔진
- 메모리 기반으로 데이터 처리
- 다양한 데이터 저장소에 저장된 데이터를 SQL로 처리 가능

### 하이브 Hive
- 하둡 기반의 데이터웨어하우징용 솔루션
- SQL과 애무 유사한 HiveQL 사용
- ㄴ HiveQL은 내부적으로 맵리듀스 잡으로 변환되어 실행됨

### HCatalog
- Pig, MapReduce, Spark에서 Hive 메타스토어 테이블에 액세스할 수 있는 도구
- 테이블을 생겅하거나 기타 작업을 수행할 수 있는 REST 인터페이스 및 커맨드라인 클라이언트 제공

### 피그 Pig
- 복잡한 맵리듀스 프로그래밍을 대체할 피그 라틴(Pig Latin)이라는 자체 언어를 제공
- 맵리듀스 API를 매우 단순화한 형태
- SQL과 유사한 형태로 설계되었음
- 다만 기존 SQL 지식을 활용하기 어려움

### 클러스터 관리
### 얀 YARN
- 데이터 처리 작업을 실행하기 위한 클러스터 자원(CPU, 메모리, 디스크 등)과 스케쥴링을 위한 프레임워크
- 기존 하둡의 맵리듀스의 단점을 극복하기 위해 시작됨
- 맵리듀스, 하이브, 임팔라, 타조, 스파크 등 다양한 애플리케이션들은 얀에서 리소스를 할당받아서 작업을 실행하게 됨

### 메소스 Mesos
- 클라우드 인프라 및 컴퓨팅 엔진의 다양한 자원(CPU, 메모리, 디스크)을 통합적으로 관리할 수 있도록 만든 자원 관리 프로젝트
- 클러스터링 환경에서 동적으로 자원을 할당하고 격리해주는 메커니즘 제공 -> 분산 환경에서 작업 실행을 최적화할 수 있음
- 1만대 이상의 노드에도 대응 가능

### 분산 서버 관리
- 클러스터에서 여러가지 기술이 이용될 때 하나의 서버에서 모든 작업이 진행되면 이 서버가 SPOF가 됨
- 이로 인한 리스크를 줄이기 위해 분산 서버 관리 기술 이용

### 주키퍼 Zookeeper
- 분산 환경에서 서버 간의 상호 조정이 필요한 다양한 서비스를 제공하는 시스템
- 역할
  - 하나의 서버에만 서비스가 집중되지 않게 서비스를 알맞게 분산해 동시에 처리하게 해줌
  - 하나의 서버에서 처리한 결과를 다른 서버와도 동기화해서 데이터의 안정성 보장
  - 운영 서버에 문제가 발생해서 서비스를 제공할 수 없는 경우, 다른 대기 중인 서버를 운영 서버로 바꿔서 서비스가 중지 없이 제공되게 함
  - 분산 환경을 구성하는 서버의 환경설정을 통합적으로 관리

### 시각화
### 휴 Hue
- 하둡과 하둡에코시스템의 지원을 위한 웹 인터페이스를 제공하는 오픈 소스
- Hive 쿼릴르 실행하는 인터페이스 제공
- 잡의 스케줄링, 잡, HDFS 등 모니터링하기 위한 인터페이스도 제공

### 보안
### 레인저 Ranger
- 하둡 클러스터의 각 모듈에 대한 보안 정책을 관리할 수 있음
- HDFS의 ACL, Hive 데이터베이스의 접근권한 등의 보안 정책과 각 모듈에 대한 접근 기록(Audit)을 보관함

### 데이터 거버넌스
- 기업의 여기저기 산재한 데이터를 같은 저장소에 관리
- 비정형 데이터를 규칙에 맞게 표준화하는 전사 차원의 빅데이터 관리 체계

### 아틀라스 Atlas
- 데이터 거버넌스로 조직이 보안/컴플라이언스 요구사항을 준수할 수 있도록 지원
- 데이터 자원에 대한 태깅, 다운스크림 데이터셋에 대한 태그전파, 메타 데이터 접그넹 대한 보안 등 다양한 기능을 가지고 있음
- 메타데이터 변경 알림 기능을 제공
- Hive, HBase, Kafka의 데이터가 변경되는 것을 알리는 기능 제공

### 아문센 Amundsen
- 데이터 디스커버리 플랫폼
- 기업에 존재하는 데이터를 검색하고 추천하는 기능을 가지고 있음
- 테이블 상세 페이지 지원

### 플룸 Flume
- 대량의 로그 데이터를 여러 소스에서 수집하여 저장하기 위한 목적으로 개발
- 데이터 수집 프레임워크
- 특징
  - 여러개의 플룸 에이전트를 연결하여 확장 가능
  - 다양한 연결 모드를 지원하여 최종 목적지에 데이터를 전달할 때 까지 유연한 구성 가능
  - 전달 받은 데이터를 메모리, File, DB에 임시 저장하여 오류 발생 시 복구 가능 -> 신뢰성 
- 구조
  - OG: master가 agent를 관리, 데이터가 집중되면 master의 병목현상 발생
    - Agent와 Collector의 설정 값을 변경해줄 때, Master도 함꼐 변경해줘야 하는 불편함이 있음 -> NG 개발
  - NG: MAster와 collector의 개념이 없고 OG의 agent 노드 구성에 해당 

### 카프카 Kafka
- 분산 스트리밍 플랫폼
- 메시징, 메트릭 수집, 로그 수집, 스트림 처리 등 다양한 용도로 사용 가능
- 특징
  - 빠름
  - 확장 가능
  - 안정적
- 발행/구독 모델
  - 주제에 맞게 브로커에게 전달하면 구독자가 브로커에 요청해서 가져가는 방식
  - 발행자는 메시지를 topic으로 카테고리화
  - 구독자는 topic에 맞는 메시지를 브로커에게 요청
  - 발행자와 구독자는 서로 알지 못함
- 발행자 Producer
  - 메세지를 생산하는 주체
  - 메세지를 만들고 브로커에게 토픽으로 분류된 메시지를 전달
    - 메시지는 배치 형태로 전달
  - 발행자는 구독자의 존재를 알지 못함
- 구독자 Consumer
  - 소비자로 메시지를 소비하는 주체
  - 발행자의 존재를 알지 못함
  - 원하는 토픽을 구독하여 스스로 조절해가면서 소비할 수 있음
  - 원하는 토픽의 각 파티션에 존재하는 오프셋의 위치를 기억하고 관리하여 데이터의 중복을 관리

### 스쿱 Sqoop
- 관계형 데이터베이스와 하둡 HDFS간에 데이터를 전송할 수 있도록 설계된 오픈소스 소프트웨어
- 버전
  - Sqoop 1: 클라이언트 방식, CLI 명령여로 작업 실행
  - Sqoop 2: 클라이언트 방식+ 서버사이드 방식 추가됨, Sqoop 서버가 존재하고, 사용자가 서버에 요청하여 작업을 실해앟는 방식
- HDFS와 RDB 간 데이터 전송
  - 양쪽으로 이동 가능
  - Hive, Pig, HBase로 이동 가능
- 동작
  - import: DB -> HDFS
  - export: HDFS -> DB

### 하둡 Hadoop
- 하나의 성능 좋은 컴퓨터를 이용하여 데이터를 처리하는 대신, 적당한 성능의 범용 컴퓨터 여러 대를 클러스터화하고, 큰 크기의 데이터를 클러스터에서 병렬로 동시에 처리하여 처리속도를 높이는 것이 목적
- 분산처리를 위한 오픈소스 프레임워크
- 구성 요소
  - Hadoop Common: 하둡의 다른 모듈을 지원하기 위한 공통 컴포넌트 모음
  - Hadoop HDFS: 분산 저장을 처리하기 위한 모듈, 여러 개의 서버를 하나의 서버처럼 묶어서 데이터 저장
  - Haddop YARN: 병렬처리를 위한 클러스터 자원관리 및 스케줄링 담당
  - Hadoop Mapreduce: 저장된 데이터를 병렬처리할 수 있게 해주는 분산처리 모듈
  - Hadoop Ozone: 하둡을 위한 오브젝트 저장소
- 장점
  - 오픈소스로 라이선스에 대한 비용 부담이 적음 시스템을 중단하지 않고, 장비의 추가가 용이(Scale Out)
  - 일부 장비에 장애가 발생하더라도 전체 시스템 사용성에 영향이 적음(Fault tolerance)
  - 저렴한 구축 비용과 비용대비 빠른 데이터 처리 
  - 오프라인 배치 프로세싱에 최적화
- 단점
  - HDFS에 저장된 데이터를 변경 불가
  - 실시간 데이터 분석 같이 신속하게 처리해야 하는 작업에는 부적합
  - 너무 많은 버전과 부실한 서포트
  - 설정의 어려움

### YARN 아키텍처
- 작업 처리 단위: 컨테이너
- 작업에 제출 -> 애플리케이션 마스터 생성 -> 애플리케이션 마스터가 리소스 매니저에 자원 요청 -> 실제 작업을 담당하는 컨테이너를 할당받아 작업 처리
- 컨테이너는 작업이 요청되면 생성됨 -> 작업이 완료되면 종료되기 때문에 클러스터 효율적 사용 가능
- MR로 구현된 작업이 아니더라도 컨테이너를 할당받아서 동작할 수 있음 -> Spark, HBase, Storm 등 다양한 컴포넌트들 실행 가능

### HDFS Hadoop Distributed File System
- 범용 하드웨어에서 동작
- 장애 복구성을 가지는 분산 파일 시스템 목표
- 실시간 처리보다는 배치 처리를 위해 설계 -> 빠른 데이터 응답시간이 필요한 작업에는 적합 x
- 네임 노드가 단일 실패 지점이 되기 때문에 네임노드 관리 중요
- 특징
  - 블록 단위 저장
    - 블록사이즈보다 작은 파일은 기존 파일 사이즈대로 저장
    - 블록 사이즈보다 크면 블록 단위로 나누어 저장 -> 단일 디스크의 데이터보다 큰 파일도 저장 가능
  - 블록 복제를 이용한 장애 복구
    - 장애 복구를 위해 각 블록을 복제하여 저장
    - 블록의 기본 복제단위는 3 -> 하나의 블록은 3개의 블록으로 복제됨
    - 같은 랙(Rack)의 서버와 다른 랙의 서버로 복제되어 저장됨
    - 블록에 문제가 생기면 복제한 다른 블록을 이용해 데이터 복구
    - 1G 데이터 저장을 위해 데이터가 복제되어 3G 저장공간 필요
  - 읽기 중심
    - 데이터를 한 번 쓰면 여러번 읽는 것을 목적으로 함
    - 파일 수정 지원 x -> 동작 단순화 -> 데이터 읽기 속도 향상
  - 데이터 지역성
    - 맵리듀스는 HDFS의 데이터 지역성을 이용해 처리 속도 증가시킴
    - 처리 알고리즘이 있는 곳에 데이터를 이동시키지 않고, 데이터가 있는 곳에서 알고리즘을 처리 -> 네트워크를 통해 대용량 데이터를 이동시키는 비용을 줄일 수 있음
- 구조
  - 마스터 슬래이브 구조: 하나의 네임노드와 여러 개의 데이터 노드로 구성
    - 네임노드: 메타 데이터 가짐
    - 데이터는 블록 단위로 나누어 데이터 노드에 저장
    - 사용자는 네임노드를 이용해 데이터를 쓰고 읽을 수 있음
  - 네임노드: 메타 데이터 관리와 데이터 노드의 관리
    - 메타 데이터 관리: 각 데이터 노드에서 전달하는 메타 데이터를 받아서 전체 노드의 메타데이터 정보의 파일 정보를 묶어서 관리
    - 메타 데이터는 사용자가 설정한 위치에 저장됨
    - 네임노드가 실행될 때 파일을 읽어서 메모리에 보관
    - 운영 중에 발생한 수정사항은 네임 노드의 메모리에 바로 적용됨
    - 데이터의 수정사항을 다음 구동시 적용을 위해서 주기적으로 Edits 파일로 저장
    - 메타데이터 파일 종류
      - Fsimage 파일: 네임 스페이스와 블록 정보
      - Edits 파일: 파일의 생성, 삭제에 대한 트랜잭션 로그, 메모리에 저장하다가 주기적으로 생성
    - 데이터 노드 관리: 네임노드는 데이터노드가 주기적으로 전달하는 하트비트와 블록리포트를 이용하여 데이터 노드의 동작상태, 블록상태 관리
      - 하트비트: 3초, dfs.heartbeat.interval
        - 하트비트를 이용하여 데이터 노드가 동작 중이라는 것을 네임노드가 알 수 있음
        - 하트비트가 도착하지 않으면 네임노드는 데이터노트가 동작하지 않는 것으로 간주 -> 더이상 IO가 발생하지 않도록 조치
      - 블록리포트: 6시간, dfs.blockreport.intervalMsec
        - 블록 리포트를 이용하여 HDFS에 저장된 파일에 대한 최신 정보 유지
        - 블록리포트에는 데이터 노드에 저장된 블록 목록과 각 블록이 로컬디스크의 어디에 저장되어 있는지에 대한 정보를 가짐
        - 네임노드의 메타데이터 갱신
  - 데이터노드: 파일을 저장하는 역할
    - 파일은 블록단위로 저장됨
    - 주기적으로 네임노드에게 하트비트와 블록리포트 전달
    - 블록파일 저장 형태
      - 블록은 블록과 블록의 메타 정보로 저장됨
      - blk_12345: 파일블록, 블록 복제 개수에 따라 동일한 이름의 블록이 여러 개의 노드에 생성됨
      - blk_12345_29082353.meta: 블록의 메타정보
    - 데이타노드 상태
      - 활성 상태
        - 데이터 노드가 Live상태인지, Dead상태인지 나타냄
        - 데이터노드가 하트비트를 주기적으로 전달하여 살아있는지 확인되면 Live 상태
        - 데이터노드에 문제가 발생하여 지정한 시간동안 하트비트를 받지 못하면 네임노드는 데이터 노드의 상태를 Stale상태로 변경
        - 이후 지정한 시간동안 응답이 없으면 Dead 노드로 변경
      - 운영 상태
        - 데이터 노드의 업그레이드, 패치 작업을 하기 위해 서비스를 잠시 멈추어야 할 경우 블록을 안전하게 보관하기 위해 설정
  - 파일 읽기/쓰기
    - 파일 읽기
      - 네임 노드에 파일이 보관된 블록 위치 요청
      - 네임노드가 블록 위치 반환
      - 각 데이터 노드에 파일 블록 요청
    - 파일 쓰기
      - 네임 노드에 파일 정보 전송하고, 파일의 블록에 써야할 노드 목록 요청
      - 네임노드가 파일을 저장할 목록 반환
      - 데이터 노드에 파일 쓰기 요청
- HDFS Federation
  - 네임노드는 파일 정보 메타데이터를 관리, 파일이 많아지면 메모리 사용량도 늘어남 -> 메모리 관리 문제를 해결하기 위해 하둡v2부터 HDFS 페더레이션 지원
  - 디렉토리(네임스페이스) 단위로 네임노드를 등록하여 사용하는 것
  - 파일, 디렉토리의 정보를 가지는 네임스페이스와 블록의 정보를 가지는 블록 풀을 각 네임노드가 독립적으로 관리
  - 네임스페이스와 블록 풀(두개 합쳐서 네임스페이스 볼륨이라고 함)을 각 네임노드가 독립적으로 관리 -> 하나의 네임노드에 문제가 생겨도 다른 네임노드에 영향 X
- HDFS 고가용성
  - HDFS는 단일 실패 지점 -> 문제 발생 시 모든 작업 중지, 파일 읽고 쓸 수 없게 됨
  - ㄴ 하둡v2에서 이 문제를 해결하기 위해 HDFS 고가용성 제공
  - 이중화된 두 대의 서버인 액티브 네임노드와 스탠바이 네임노드를 이용하여 지원
  - 액티브, 스탠바이는 데이터 노드로부터 블록 리포트와 하트비트를 모두 받아서 동일한 메타데이터 유지, 공유 스토리지를 이용하여 에디트파일 공유
  - 액티브 네임노드는 네임노드의 역할 수행
  - 스탠바이 네임노드는 액티브와 동일한 메타데이터 정보를 유지하다가 액티브에 문제가 발생하면 스탠바이가 액티브 역할을 함
  - ㄴ 액티브 네임노드에 문제가 발생한 것을 자동으로 확인하기 어렵기 때문에 보통 주피퍼를 이용하여 장애 발생시 자동으로 변경될 수 있도록 함
- HDFS 세이프모드
  - 데이터노드를 수정할 수 없는 상태
  - 세이프 모드가 되면 데이터는 읽기 전용 상태가 됨, 데이터 추가와 수정 불가능, 복제도 일어나지 않음
  - 사용 시기: 관리자가 서버 운영 정비(수동), 네임노드에 문제가 생겨서 정상적인 동작을 할 수 없을 때(자동)
- 밸런서 balance
  - 데이터 불균형이 발생하면 실행
  - 데이터 불균형이 발생하는 경우
    - 데이터 노들르 추가하는 경우
    - 대량의 데이터를 삭제하는 경우 -> 데이터 노드간 저장공간 차이가 20~30% 차이나는 경우
    - 대량의 데이터를 추가하는 경우 -> 특정 데이터 노드에 데이터가 적은 경우, 네임노드는 데이터 저장공간이 작은 노드를 우선적으로 사용하는데 이 경우 특정 노드로 I/O가 집중됨
  - 밸런서는 작업간 많은 데이터 이동이 발생하기 때문에 대역폭을 지정하여 다른 작업에 영향이 가지 않도록 권장

### 맵리듀스 Mapreduce
- 간단한 단위 작업을 반복하여 처리할 때 사용하는 프로그래밍 모델 -> 하둡에서 분산처리 담당
- Map: 간단한 단위 작업을 처리
- Reduce: 맵 작업의 결과물을 모아서 집계
- 맵, 리듀스 작업은 병렬로 처리 가능한 직업으로 여러 컴퓨터에서 동시에 작업을 처리하여 속도를 높일 수 있음
- 작업단위: 잡(하둡v1) = 애플리케이션(하둡v2)
  - YARN 아키텍처가 도입되면서 이름은 변경되었지만 동일하게 관리됨
  - 잡은 맵 태스크와 리듀스 태스크로 나누어짐 -> 태스크는 attempt 단위로 실행됨
- 작업 종류
  - 리듀서 작업이 있는 경우
    - 리듀서가 하나인 경우: 정렬 -> 리듀서 하나로 모든 작업을 처리해서 시간이 오래 걸림
    - 리듀서가 여러 개인 경우: 집계 -> 리듀서의 수 만큼 파일이 생성됨, HDFS의 부하를 방지하기 위해 추가적인 파일 머지 작업이 필요할 수도 있음 
  - 리듀서 작업이 없는 경우 = 매퍼만 있는 작업: 파일을 읽어서 바로 쓰는 작업(리듀스 필요 없음)
    - 원천 데이터를 읽어서 가공하고 바로 쓴느 경우
    - 리듀서 작업이 없기 때문에 빠름
    - 매퍼의 수만큼 파일이 생성되기 때문에 추가적인 파일 머지 작업이 필요할 수 있음
- 처리 단계
  1. 입력: 텍스트, csv, gzip 형태의 데이터를 읽어서 맵으로 전달
  2. 맵 Map: 입력을 분할하여 키별로 데이터를 처리
  3. 컴바이너 Combiner(로컬 리듀서): 네트워크를 타고 넘어가는 데이터르 줄이기 위해 맵의 결과 정리
  4. 파티셔너 Partitioner: 맵의 출력 결과 키 값을 해쉬 처리하여 어떤 리듀서로 넘길지 결정
  5. 셔플 Shuffle: 각 리듀서로 데이터 이동
  6. 정렬 Sort: 리듀서로 전달된 데이터를 키 값 기준으로 정렬
  7. 리듀서 Reduce: 리듀서로 데이터를 처리하고 결과를 저장
  8. 출력: 리듀서의 결과를 정의된 형태로 저장

### YARN
- 하둡2에서 도입한 클러스터 리소스 관리 및 애플리케이션 라이프 사이클 관리를 위한 아키텍처
- 구성
  - 자원 관리
    - 리소스 매니저: 노드매니저로부터 전달받은 정보를 이용하여 클러스터 전체의 자원 관리, 자원 사용 상태 모니터링, 애플리케이션 마스터에서 자원 요청 시 비어 있는 자원 사용할 수 있도록 처리
    - 노드매니저: 클러스터의 각 노드마다 실행됨, 현재 노드의 자원상태를 관리하고 리소스매니저에 현재 자원상태 보고
  - 애플리케이션 라이프 사이클 관리
    - 애플리케이션 마스터: 작업 실해을 위한 자원을 리소스 매니저에 요청, 자원을 할당받아서 각 노드에 컨테이너 실행, 실제 작업 진행
    - 컨테이너: 실제 작업이 실행되는 단위, 작업이 종료되면 결과를 애플리케이션 마스터에게 알림
- 스케줄러: 자원을 분배하는 규칙을 설정하는 것
  - FIFO 스케줄러
    - 먼저 들어온 작업이 먼저 처리, 먼저 들어온 작업이 종료될 때까지 다음작업은 대기
    - 자원 효율적 사용 X -> 테스트 목적으로만 사용하는 것이 좋음
  - Fair 스케줄러
    - 제출된 작업이 동등하게 리소스를 점유
    - 작업 큐에 작업이 제출되면 클러스터는 자원을 조절하여 작업에 균등하게 자원 할당
    - 메모리와 CPU를 기반으로 자원 설정 가능
  - Capacity 스케줄러
    - 하둡v2의 기본 스케줄러
    - 트리 형태로 계층화된 큐 선언
    - 큐별로 사용가능한 용량을 할당하여 자원 관리
    - 만약 클러스터 자원에 여유가 있다면, 설정을 이용하여 각 큐에 설정된 용량 이상의 자원을 이용하게 할 수도 있고, 운영 중에도 큐를 추가할 수 있는 유연성도 있음
- HA 기능
  - YARN은 리소스 매니저가 단일 실패 지점 -> 리소스 매니저에 문제가 발생하면 클러스터의 자원관리, 작업관리 기능을 사용할 수 없기 때문에 하둡 2.4버전부터 HA기능 제공
  - 리소스 매니저 고가용성은 주키퍼와 액티브, 스탠바이 리소스 매니저를 이용하여 제공

### 하이브 Hive
- 하둡 에코시스템 중에서 데이터를 모델링하고 프로세싱하는 경우 가장 많이 사용하는 데이터 웨어하우징용 솔루션
- RDB의 데이터베이스, 테이블과 같은 형태로 HDFS에 저장된 데이터의 구조를 정의하는 방법 제공
- 이 데이터를 대상으로 SQL과 유사한 HiveQL 쿼리를 이용하여 데이터를 조회하는 방법 제공
- 구성 요소
  - UI: 사용자가 쿼리 및 기타 작업을 시스템에 제출하는 사용자 인터페이스 ex. CLI, Beeline, JDBC 등
  - Driver: 쿼리를 입력받고 작업을 처리, 사용자 세션을 구현하고 JDBC/ODBC 인터페이스 API 제공
  - Compiler: 메타 스토어를 참고하여 쿼리 구문을 분석하고 실행계획을 생성
  - Metastore: 디비, 테이블, 파티션의 정보를 저장
  - Execution Engine: 컴파일러에 의해 생성된 실행 계획을 실행
- 데이터베이스
  - 테이블의 이름을 구별하기 위한 네임 스페이스 역할
  - 테이블의 데이터의 기본 저장 위치를 제공
- 함수
  - UDF: 1개의 열을 처리하여 1개의 열을 반환하는 함수
  - UDAF: N개의 열을 이용하여, 1개의 열을 반환
  - UDTF: 1개의 열을 입력받아 N개의 열을 반환
  - 사용자 정의 함수
    - 하이브에서 제공하는 기본 함수 구현을 상속하여 Java로 구현
    - UDF, UDAF, UDTF를 각각 구현할 수 있음
    - 파이썬 스크립트를 이용하여 UDTF를 구현할 수도 있음
- 트랜잭션
  - 작업의 논리적인 단위, 동시성 지원
  - 파일 수정/변경이 없는 HDFS 특성상 모든 기능이 완벽하게 지원되지 않고 다음의 기능만 제공
    - BEGIN, COMMIT, ROLLBACK은 아직 지원하지 않음, 현재는 auto-commit 만 지원 
    - ORC 파일 포맷, 버켓팅 설정이 된 매니지드 테이블에서만 지원 
    - Non-ACID 세션에서는 ACID 테이블에 접근 불가
  - 기본 트랜잭션 설정은 off
  - 처리 순서
    - HDFS는 트랜잭션을 지원하기 위해서 데이터를 베이스 파일에 기록
    - 트랜잭션(생성/수정/삭제)이 발생할때마다 델타delta 파일에 내용 기록
    - 파일을 읽을 때 베이스 파일에 델타 파일의 내용을 적용하여 수정된 내용을 반환
    1. 테이블이나 파티션은 베이스 파일의 집합으로 저장 
    2. insert, update, delete 에 대해서는 델타 파일로 저장 
    3. 읽는 시점에 베이스 파일과, 델터 파일을 합쳐서 수정된 내용을 반환
  - 락 Lock
    - 동시성 지원, 트랜잭션을 처리할 때 테이블, 파티션에 접근을 제어하는 용도로 사용
    - 종류
      - 공유 잠금(S, Shared) = 읽기 잠금(Read Lock): 다른 트랜잭션에서 데이터를 읽으려고 할 때 다른 공유 잠금은 허용되지만, 배타적 감금은 허용되지 않음
      - 배타적 잠금(X, Exclusive) = 쓰기 잠금(Write Lock): 데이터를 변경하려고 할때 다른 트랜잭션에서 데이터를 읽거나 변경하지 못학 배타적 잠금을 설정, 배타적 잠금이 걸리면 공유잠금, 배타적 잠금을 설정할 수 없음
    - 락의 획득
      - 락은 논파티션 테이블과 파티션 테이블에서 따로 동작
      - 논파티션 테이블은 직관적으로 동작
        - 테이블을 읽을 때는 S 잠금 획득
        - 다른 작업에서는 X잠금 획득
      - 파티션 테이블
        - 읽을 때 테이블에 S 잠금
        - 파티션에 S 잠금 획득
        - 다른 작업에서는 테이블에 S잠금, 파티션에 X잠금 획득
- 성능 최적화
  - 작업엔진 선택: TEZ 엔진 사용
    - 맵리듀스(MR) 엔진은 연산의 중간 파일을 로컬 디스크에 쓰면서 진행하여 이로인한 잦은 IO처리로 작업이 느려짐
    - 테즈(TEZ)엔진은 작업 처리결과를 메모리에 저장하여 맵리듀스보다 빠른 속도로 작업을 처리할 수 있음
  - 파일 저장 포맷: ORC 파일 사용
    - 테이블의 데이터 저장에 ORC 파일을 사용하여 처리속도를 높일 수 있음
    - ORC 파일 포맷은 데이터를 컬럼 단위로 저장하기 때문에 거맥 속도가 빠르고 압축률이 높음
  - 데이터 처리방식: 벡터화(Vectorization) 사용
    - 벡터화 처리는 한번에 1행을 처리하지 않고, 한 번에 1024행을 처리하여 속도를 높이는 기술
    - ORC 파일 포맷에서만 사용 가능
    - 필터링, 조인, 집합 연산에서 40~50% 정도의 성능 향상 기대할 수 있음
  - 데이터 저장 효율화: 파티셔닝, 버케팅 사용
    - 하이브는 디렉토리 단위로 데이터를 처리하기 때문에 검색에 사용되는 데이터를 줄이기 위한 방안으로 파티셔닝, 버케팅 기능을 사용하면 좋음
    - 파티셔닝: 데이터를 폴더 단위로 구분하여 저장하고
    - 버케팅: 지정한 개수의 파일에 컬럼의 해쉬 값을 기준으로 데이터를 저장
    - 파티셔닝, 버케팅을 이용하여 한 번에 읽을 데이터의 크기를 줄일 수 있음
  - 통계정보 이용: 하이브 stat 사용
    - 하이브는 테이블, 파티션의 정보를 메타스토어에 저장하고 조회나 count, sum 같은 집계함수를 처리할 때 이 정보를 이용할 수 있음
    - 맵리듀스 연산 없이 바로 작업할 수 있기 때문에 작업의 속도가 빨라짐
  - 옵티마이저 사용: CBO
    - 하이브는 카탈리스트 옵티마이저를 이용하여 효율적으로 작업을 처리할 수 있음
    - explain을 이용하여 작업 분석 상태를 확인할 수 있음
  - YARN: 작업 큐 설정
    - YARN의 스케줄러 설정을 통해 작업의 효율성을 높일 수 있음
    - 작업의 성격에 따라 큐를 여러 개 만들어서 스케줄러의 사용 설정을 적용하면 좋음
    - 하나의 큐에 모든 작업을 넣지 않고, batch, adhoc 같은 형태로 큐를 만들어서 큐의 최대 사용량 설정을 통해 적절하게 작업을 분산하여 주는 것이 좋음