# 스파크 완벽 가이드

# Part 1: 빅데이터와 스파크 간단히 살펴보기
## 1장 아파치 스파크란
- 아파치 스파크: 통합 컴퓨팅 엔진, 클러스터 환경에서 데이터를 병렬로 처리하는 라이브러리 집합

### 1.1 아파치 스파크의 철학
- 통합: 스파크는 간단한 데이터 읽기에서부터 SQL 처리, 머신러닝, 스트림 처리를 같은 연산 엔진과 일관성 있는 API로 수행할 수 있도록 설계
- 컴퓨팅 엔진
  - 저장소 시스템의 데이터를 연산하는 역할만 수행하고 영구 저장소 역할은 수행하지 않음
  - 스파크는 내부에 데이터를 오랜 시간 저장하지 않고 특정 저장소 시스템을 선호하지 않음

## 2장 스파크 간단히 살펴보기
### 2.1 스파크의 기본 아키텍처
#### 2.1.1 스파크 애플리케이샨
- 드라이버 프로세스
  - 클러스터 노드 중 하나에서 실행됨
  - main함수를 실행함
  - 스파크 애플리케이션 정보의 유지 관리, 사용자 프로그램이나 입력에 대한 응답, 전반적인 익스큐터 프로세스의 작업과 관련된 분석, 배포, 스케줄링 역할
  - 스타크 애플리케이션의 수명 주기 동안 관련 정보 모두 유지
  - 주어진 작업을 완료하기 위해 드라이버 프로그램의 명령을 익스큐터에서 실행할 책임이 있음
- 익스큐터 프로세스
  - 드라이버 프로세스가 할당한 작업을 수행
  - 드라이버가 할당한 코드를 실행하고 진행상황을 다시 드라이버 노드에 보고하는 역할 수행
  - 스파크 코드를 실행하는 역할
- 클러스터 매니저
  - 스파크 스탠드얼론 클러스터 매니저, 하둡 YARN, 메소스 중 하나 선택 가능
  - 하나의 클러스터에서 여러 개의 스파크 애플리케이션 실행 가능
  - 스파크는 사용가능한 자원을 파악하기 위해 클러스터 매니저를 사용

### 2.2 스파크의 다양한 언어 API
- 사용자는 스파크 코드를 실행하기 위해 SparkSession 객체를 진입점으로 사용할 수 있음
- 파이썬이나 R로 스파크를 사용할 때는 JVM 코드를 명시적으로 작성 X
- 스파크는 사용자를 대신해 파이썬이나 R로 작성한 코드를 익스큐터의 JVM에서 실행할 수 있는 코드로 변환함

### 2.3 스파크 API
- 다양한 언어로 스파크를 사용할 수 있는 이유는 스파크가 기본적으로 두가지 API를 제공하기 때문
  - 저수준 비구조적 API
  - 고수준 구조적 API

### 2.5 SparkSession
- 스파크 애플리케이션은 SparkSession이라고 불리는 드라이버 프로세스를 제어함
- SparkSession 인스턴스
  - 사용자가 정의한 처리 명령을 클러스터에서 실행
  - 하나의 SparkSession은 하나의 스파크 애플리케이션에 대응

### 2.6 DataFrame
- 가장 대표적인 구조적 API
- 스키마: 컬럼과 컬럼의 타입을 정의한 목록
- 스파크 DataFrame은 수천 대의 컴퓨터에 분산되어 있음
- Pandas나 R의 DataFrame에는 분산 컴퓨터가 아닌 단일 컴퓨터에 존재함 -> 데이터프레임으로 수행할 수 있는 작업이 해당 머신이 가진 자원에 따라 제한될 수 밖에 없음

### 2.6.1 파티션
- 스파크는 모든 익스큐터가 병렬로 작업을 수행할 수 있도록 파티션이라 불리는 청크 단위로 데이터를 분할함
- 파티션: 클러스터의 물리적 머신에 존재하는 로우의 집합을 의미
- 데이터프레임의 파티션은 실행 중에 데이터가 컴퓨터 클러스터에서 물리적으로 분산되는 방식을 나타냄
  - 만약 파티션이 하나라면 스파크의 수천 개의 익스큐터가 있더라도 병렬성은 1이 됨
  - 수백 개의 파티션이 있더라도 익스큐터가 하나 밖에 없다면 병렬성은 1이됨
- 데이터프레임을 사용하면 파티션을 수동 혹은 개별적으로 처리할 필요가 없음
- 물리적 파티션에 데이터 변환용 함수를 지정하면 스파크가 실제 처리 방법을 결정함

## 2.7 트랜스포메이션
- 스파크 핵심 데이터 구조는 불변성 -> 한번 생성하면 변경할 수 없음
- 데이터프레임을 변경하려면 원하는 변경 방법을 스파크에게 알려줘야 함 -> 이때 사용하는 명령을 트랜스포메이션이라고 함
- 변수에 값을 저장하는 코드를 실행해도 결과는 출력되지 않음 -> 추상적인 트랜스포메이션만 지정한 상태이기 때문에 액션을 호출하지 않으면 스파크는 실제 트랜스포메이션을 실행하지 않음
- 트랜스포메이션 유형
  - 좁은 의존성
    - 각 입력 파티션이 하나의 출력 파티션에만 영향을 미침
    - where 구문은 좁은 의존성을 가짐 
    - 하나의 파티션이 하나의 출력 파티션에만 영향을 미침
    - 스파크에서 파이프라이닝을 자동으로 수행 -> 데이터프레임에 여러 필터를 지정하는 경우 모든 작업이 메모리에서 일어남
  - 넓은 의존성
    - 하나의 입력 파티션이 여러 출력 파티션에 영향을 미침
    - 클러스터에서 파티션을 교환하는 셔플이 동작하여 셔플의 결과를 디스크에 저장함

### 2.7.1 지연 연산
- 지연 연산 lazy evaluation: 스파크가 연산 그래프를 처리하기 직전까지 기다리는 동작 방식을 의미
- 스파크는 특정 연산 명령이 내려진 즉시 데이터를 수정하지 않고 원시 데이터에 적용할 트랜스포메이션의 실행 계획을 생성함
- 스파크는 코드를 실행하는 마지막 순간까지 대기하다가 원형 데이터프레임 트랜스포메이션을 간결한 물리적 실행 계획으로 컴파일함
- 스파크는 이 대기 과정을 거치며 전체 데이터 흐름을 최적화하는 엄청난 강점을 가지고 있음
- 예시: 데이터 프레임의 조건절 푸시다운 predicate pushdown
- 아주 복잡한 스파크 잡이 원시 데이터에서 하나의 로우만 가져오는 필터를 가지고 있다면 필요한 레코드 하나만 읽는 것이 효율적 -> 스파크는 이 필터를 데이터소스로 위임하는 최적화 작업을 자동을 수행

## 2.8 액션
- 사용자는 트랜스포메이션을 사용해 논리적 실행 계획을 세울 수 있음
- 하지만 실제 연산을 수행하려면 액션 명령을 내려야 함
- 액션은 일련의 트랜스포메이션으로부터 결과를 계산하도록 지시하는 명령
- 액션 유형
  - 콘솔에서 데이터를 보내는 액션
  - 각 언어로 된 네이티브 객체에 데이터를 모으는 액션
  - 출력 데이터소스에 저장하는 액션
- 액션을 지정하면 스파크 잡이 시작됨
- 스파크 잡은 필터(좁은 트랜스포메이션)를 수행한 후 파티션별로 레코드 수를 카운트(넓은 트랜스포메이션)함
- 그리고 각 언어에 적합한 네이티브 객체를 모음
- 이때 스파크가 제공하는 스파크 UI로 클러스터에서 실행중인 스파크 잡을 모니터링할 수 있음

## 2.9 스파크UI
- 스파크 잡의 진행 상황을 모니터링할 때 사용
- 드라이버 노드의 4040 포트로 접속 가능
- 스파크 잡 상태, 환경 설정, 클러스터 상태 등 확인
- 스파크 잡을 튜닝하고 디버깅할 때 매우 유용
- 스파크 잡은 개별 액션에 의해 트리거되는 다수의 트랜스포메이션으로 이루어져 있으며 스파크 UI로 잡을 모니터링할 수 있음

### 2.10.1 Dataframe과 SQL
- 스파크는 언어 상관없이 같은 방식으로 트랜스포메이션을 실행할 수 있음
- 스파크는 SQL쿼리를 데이터프레임 코드와 같은 실행 계획으로 컴파일하므로 둘 사이의 성능의 차이가 없음
- 스파크는 해당 데이터프레임이나 자신의 원본 데이터프레임에 액션이 호출되기 전까지 데이터를 읽지 않음

# 3장 스파크 기능 둘러보기
## 3.1 운영용 애플리케이션 실행하기
- spark-submit 명령
  - 대화형 셸에서 개발한 프로그램을 운영용 애플리케이션으로 쉽게 전환 가능
  - 애플리케이션 코드를 클러스터에 전송해 실행시키는 역할
  - 애플리케이션 실행에 필요한 자원과 실행 방식, 옵션을 지정 가능
  - master 옵션의 인숫값을 변경하면 스파크가 지원하는 스파크 스탠드얼론, 메소스, YARN 클러스터 매니저에서 동일한 애플리케이션 실행 가능

## 3.2 Dataset: 타입 안정성을 제공하는 구조적 API
- Dataset
  - 자바와 스칼라의 정적 데이터 타입에 맞는 코드 -> 정적 타입 코드를 지원하기 위해 고안된 스파크의 구조적 API
  - 타입안정성을 지원하며 동적타입 언어인 파이썬과 R에서는 사용 X
  - 데이터프레임의 레코드를 사용자가 자바나 스칼라로 정의한 클래스에 할당하고 자바의 ArrayList 또는 스칼라의 Seq 객체 등의 고정 타입형 컬렉션으로 다룰 수 있는 기능 제공
  - 장점
    - 필요한 경우에 선택적으로 사용 가능
    - 스파크가 제공하는 여러 함수를 이용해 추가 처리 작업 가능
    - collect 메서드나 take 메서드를 호출하면 Dataset에 매개변수로 지정한 타입의 객체를 반환
    - 코드 변경 없이 타입 안정성을 보장, 로컬이나 분산 클러스터 환경에서 데이터를 안전하게 다룰 수 있음

## 3.3 구조적 스트리밍
- 구조적 스트리밍: 스파크 2.2 버전에서 안정화된 스트림 처리용 고수준 API
- ㄴ 사용하면 구조적 API로 개발된 배치 모드의 연산을 스트리밍 방식으로 실행 가능, 지연 시간을 줄이고 증분 처리 가능
- 배치 처리용 코드를 일부 수정하여 스트리밍 처리를 수행하고 값을 빠르게 얻을 수 있다는 장점

## 3.4 머신러닝과 고급 분석
- 스파크에서 머신러닝 모델을 학습시키는 과정
  1. 아직 학습되지 않은 모델 초기화 -> 학습 전 알고리즘 명칭: Algorithm
  2. 해당 모델 학습: AlgorithmModel

# 4장 구조적 API 개요
## 4.1 DataFrame과 Dataset
- 스파크는 DataFrame과 Dataset이라는 두 가지 구조화된 컬렉션 개념을 가지고 있음
  - 잘 정의된 로우와 컬럼을 가지는 분산 테이블 형태의 컬렉션
  - 각 컬럼은 다른 컬러과 동일한 수의 로우를 가져야 함
  - 컬렉션의 모든 로우는 같은 데이터 타입 정보를 가지고 있어야 함
  - 결과를 생성하기 위해 어떤 데이터에 어떤 연산을 적용해야 하는지 정의하는 지연 연산의 실행 계획, 불변성을 가진
- DataFrame에 액션을 호출하면 스파크는 트랜스포메이션을 실제로 실행하고 결과를 반환함
- 스키마는 분산 컬렉션에 저장할 데이터 타입을 정의하는 방법

## 4.2 스키마
- DataFrame의 컬럼명과 데이터 타입을 정의
- 스키마는 데이터 소스에서 얻거나(Schema on read) 직접 정의
- 스키마는 여러 데이터 타입으로 구성되므로 어떤 타입이 어느 위치에 있는지 정의하는 방법 필요

## 4.3 스파크의 구조적 데이터 타입 개요
- 스파크는 실행 계획 수립과 처리에 사용하는 자체 데이터 타입 정보를 가지고 있는 카탈리스트 엔진을 사용함
  - 카탈리스트 엔진은 다양한 실행 최적화 기능을 제공
- 스파크는 자체 데이터 타입을 지워하는 여러 언어 API와 직접 매핑되며, 각 언어에 대한 매핑 테이블을 가지고 있음

### 4.3.1 Dataframe과 Dataset 비교
- Dataframe
  - 비타입형
  - 스키마에 명시된 데이터 타입의 일치 여부를 런타임이 되어서 확인
  - Row 타입으로 구성된 Dataset
    - Row타입은 스파크가 사용하는 연산에 최적화된 인메모리 포맷의 내부적인 표현 방식
    - Row타입을 사용하면 가비지 컬렉션과 객체 초기화 부하가 있는 JVM 데이터 타입을 사용하는 대신 자체 데이터 포맷을 사용하기 때문에 매우 효율적인 연산 가능
    - 파이썬이나 R에서는 스파크의 Dataset을 사용할 수 없지만 최적화된 포맷인 Dataframe 사용 가능
  - Dataframe을 사용하면 스파크의 최적화된 내부 포맷 사용 가능 -> 내부 포맷 사용하면 스파크가 지원하는 어떤 언어 API를 사용하더라도 동일한 효과와 효율성을 얻을 수 있음
- Dataset
  - 타입형
  - 스키마에 명시된 데이터 타입의 일치 여부를 컴파일 타임에 확인
  - JVM 기반의 언어인 스칼라와 자바에서만 지원
  - 데이터 타입을 정의하려면 스칼라의 케이스 클래스나 자바 빈을 사용해야 함

### 4.3.2 컬럼
- 컬럼은 정수형,문자열 같은 단순 데이터 타입, 배열,맵 같은 복합 데이터 타입, null 값을 표현

### 4.3.3 로우
- 로우는 데이터 레코드
- Dataframe의 레코드는 Row타입으로 구성된
- 로우는 SQL, RDD, 데이터소스에서 얻거나 직접 만들 수 있음 

## 4.4 구조적 API의 실행 과정
- 구조적 API 쿼리가 사용자 코드에서 실제 실행 코드로 변환되는 과정
1. DataFrame/Dataset/SQL을 이용해 코드를 작성
2. 정상적인 코드라면 스파크가 논리적 실행계죅으로 변환
3. 스파크는 논리적 실행계획을 물리적 실행 계획으로 변환하며 그 과정에서 추가적인 최적화를 할 수 있는지 확인
4. 스파크는 클러스터에서 물리적 실행 계획(RDD처리)을 실행

### 4.4.1 논리적 실행 계획
- 추상적 트랜스포메이션만 표현
- 드라이버나 익스큐터 정보 고려하지 않음
- 사용자의 표현식을 최적화된 버전으로 변환 -> 사용자 코드는 검증 전 논리적 실행 계획으로 변환됨
- 코드의 유효성과 테이블이나 컬럼의 존재여부만을 판단하는 과정이므로 아직 실행 계획을 검증하지는 않은 상태
- 스파크 분석기는 컬럼과 테이블을 검증하기 위해 카탈로그, 모든 테이블의 저장소, Dataframe 정보 활용
- 필요한 테이블이나 컬럼이 카탈로그에 없다면 논리적 실행계획이 만들어지지 않음
- 테이블과 컬럼에 대한 검증 결과는 카탈리스트 옵티마이저로 전달됨
- 카탈리스트 옵티마이저는 조건절 푸시다운이나 선택절 구문을 이용해 논리적 실행 계획을 최적화하는 규칙의 모음

### 4.4.2 물리적 실행 계획
- 스파크 실행계획 = 물리적 실행 계획
- 논리적 실행 계획을 클러스터 환경에서 실행하는 방법을 정의
- 물리적 실행 계획은 일련의 RDD와 트랜스포메이션으로 변환됨

### 4.4.3 실행
- 저수준 프로그래밍 인터페이스인 RDD를 대상으로 모든 코드를 실행
- 스파크는 런타임에 전체 태스크나 스테이지를 제거할 수 있는 자바 바이트 코드를 생성해 추가적인 최적화를 수행
- 처리결과를 사용자에게 반환

# 5장 구조적 API 기본 연상
- DataFrame
  - Row 타입의 레코드
  - 각 레코드에 수행할 연산 표현식을 나타내는 여러 컬럼
- 스키마
  - 각 컬럼명과 데이터 타입 정의
- 파티셔닝
  - DataFrame이나 Dataset이 클러스터에서 물리적으로 배치되는 형태를 정의
- 파티셔닝 스키마
  - 파티션을 배치하는 방법을 정의
  - 파티셔닝의 분할 기준은 특정 컬럼이나 비결정론적 값을 기반으로 설정할 수 있음

## 5.1 스키마
- 데이터프레임의 컬럼명과 데이터 타입 정의
- 데이터소스에서 스키마를 얻거나 직접 정의할 수 있음
- 여러 개의 StructField 타입으로 구성된 StructType 객체
- StructField는 이름, 데이터 타입, 컬럼이 값이 없거나 null일 수 있는지 지정하는 불리언값을 가짐
- 필요한 경우 컬럼과 관련된 메타데이터를 지정할 수도 있음
- 스키마는 복합 데이터 타입인 StructType을 가질 수 있음
- 스파크는 런타임에 데이터 타입이 스키마의 데이터 타입과 일치하지 않으면 오류를 발생시킴
- 스파크는 자체 데이터 타입 정보를 사용하므로 프로그래밍 언어의 데이터 타입을 스파크의 데이터 타입으로 설정할 수 없음
 
## 5.2 컬럼과 표현식
- 표현식으로 데이터프레임의 컬럼을 선택, 조작, 제거할 수 있음
- 스파크의 컬럼은 표현식을 사용해 레코드 단위로 계산한 값을 단순하게 나타내는 논리적 구조 -> 실젯값을 얻으려면 로우가 필요하고 로우를 얻으려면 데이터프레임이 필요
- 컬럼을 수정하려면 데이터프레임의 스파크 트랜스포메이션을 사용해야 함

### 5.2.1 컬럼
- 컬럼은 컬럼명을 카탈로그에 저장된 정보와 비교하기 전까지 미확인 상태로 남음
- col 메서드를 사용해 명시적으로 컬럼을 정의하면 스파크는 분석기 실행 단계에서 컬럼 확인 절차를 생략

### 5.2.2 표현식
- 표현식은 데이터프레임 레코드의 여러 값에 대한 트랜스포메이션 집합을 의미
- 여러 컬럼명을 입력으로 받아 식별하고 단일 값을 만들기 위해 다양한 표현식을 각 레코드에 적용하는 함수
- 단일 값은 Map이나 Array 같은 복합 데이터 타입일 수도 있음
- 표현식은 expr 함수로 사용 가능 -> expr() col() 구문은 동일하게 동작함
- 컬럼은 단지 표현식일 뿐
- 컬럼과 컬럼의 트랜스포메이션은 파싱된 표현식과 동일한 논리적 실행계획으로 컴파일 됨

## 5.3 레코드와 로우
- 스파크에서 데이터프레임의 각 로우는 하나의 레코드
- 스파크는 레코드를 Row 객체로 표현
- 스파크는 값을 생성하기 위해 컬럼 표현식으로 Row 객체를 다룸
- Row 객체는 내부에 바이트 배열을 가짐
- 이 바이트 배열 인터페이스는 오직 컬럼 표현식으로만 다룰 수 있이므로 사용자에게 절대 노출되지 않음

### 5.3.1 로우 생성하기
- 각 컬럼에 해당하는 값을 사용해 Row 객체를 직접 생성할 수 있음
- Row 객체는 스키마 정보가 없고 데이터프레임만 유일하게 스키마를 갖음
- 그러므로 Row 객체를 직접 생성하려면 데이터프레임의 스키마와 같은 순서로 값을 명시해야 함

## 5.4 데이터프레임의 트랜스포메이션
- 데이터프레임을 다루는 주요 방법 -> 트랜스포메이션으로 변환 가능 
  - 로우나 컬럼 추가
  - 로우나 컬럼 제거
  - 로우를 컬럼으로 변화나거나 그 반대로 변환
  - 컬럼값을 기준으로 로우 순서 변경
- 가장 일반적인 트랜스포메이션은 특정 컬럼 값을 변경하고 그 결과를 반환하는 것

### 5.4.1 데이터프레임 생성하기
- 컬럼이나 표션식을 사용하는 select 
- 문자열 표현식을 사용하는 selectExpr
- 메서드로 사용할 수 없는 org.apache.spark.sql.functions 패키지에 포함된 다양한 함수

### 5.4.2 select와 selectExpr
- select와 selectExpr 이용하면 데이터 테이블에 SQL을 실행하는 것처럼 데이터프레임에서도 SQL 사용 가능
- select에 expr 함수를 사용하는 패턴 -> selectExpr 

### 5.4.3 스파크 데이터 타입으로 변환하기
- 새로운 컬럼이 아닌 명시적인 값을 스파크에 전달해야 할 때 리터럴 사용
- 리터럴은 프로그래밍언어의 리터럴 값을 스파크가 이해할 수 있는 값으로 변환

### 5.4.4 컬럼 추가하기
- 데이터프레임에 신규 컬럼을 추가하는 공식적인 방법은 withColumn 사용
- withColumn 인수: 컬럼명, 값을 생성할 표현식
- withColumn, expr로 컬럼명 변경 가능 

### 5.4.6 예약 문자와 키워드
- 공백이나 하이픈같은 예약 문자는 컬럼명에 사용 X
- 예약 문자를 컬럼명에 사용하려면 백틱(`)문자를 이용해 이스케이핑해야 함

### 5.4.7 대소문자 구분
- 기본적으로 스파크는 대소문자를 가리지 않음
- 설정을 통해서 구분하게 만들 수 있음

### 5.4.10 로우 필터링하기
- 스파크는 자동으로 필터의 순서 상관없이 동시에 모든 필터링 작업을 수행하기 때문에 항상 유용한 것은 아님
- 그러므로 여러개의 AND 필터를 지정하려면 차례대로 필터를 연결하고 판단은 스파크에게 맡겨야 함

### 5.4.12 무작위 샘플 만들기
- sample메서드 이용
- 표본 데이터 추출 비율 지정 가능, 복원추출, 비복원 추출의 사용여부 지정 가능

### 5.4.13 임의 분할하기
- 임의 분할: 원본 데이터프레임을 임의의 크기로 분할할 때 사용
- 머신러닝 알고리즘에서 학습셋, 검증셋, 테스트셋 만들 때 주로 사용
- randomSplit 함수는 임의성을 가지도록 설계되었으므로 반드시 시드값을 설정해야 함

### 5.4.14 로우 합치기와 추가하기
- 데이터프레임은 불변성을 가짐 -> 데이터프레임에 레코드를 추가하는 작업은 데이터프레임을 변경하는 작업이기 때문에 불가능함
- 데이터 프레임에 레코드를 추가하려면 원본 데이터프레임을 새로운 데이터프레임과 통합해야 함
- 통합은 두 개의 데이터프레임을 단순히 결합하는 것 -> 두 개의 데이터 프레임은 반드시 동일한 스키마와 컬럼수를 가져야 함

### 5.4.15 로우 정렬하기
- sort와 orderBy는 완전히 같은 방식으로 동작(orderby 메서드 내부에서 sort 메서드 호출)

### 5.4.16 로우 수 제한하기
- limit 함수 이용

### 5.4.17 repartition과 coalesce
- 최적화 기법 중 하나는 자주 필터링하는 컬럼을 기준으로 데이터를 분할하는 것
- 이를 통해 파티셔닝 스키마와 파티션 수를 포함해 클러스터 전반의 물리적 데이터 구성을 제어할 수 있음
- repartition
  - 호출하면 무조건 전체 데이터를 셔플
  - 향후에 사용할 파티션 수가 현재 파티션 수 보다 많거나 컬럼을 기준으로 파티션을 만드는 경우에만 사욯해야 함
  - 특정 컬럼을 기준으로 자주 필터링한다면 필터링되는 컬럼을 기준으로 파티션을 재분배하는 것이 좋음
  - 선택적으로 파티션 수 지정 가능
- coalesce
  - 전체 데이터를 셔플하지 않고 파티션을 병합하려는 경우에 사용 
  - 파티션 수를 줄이려면 셔플이 일어나는 repartition 대신 coalesce 사용해야 함

# 6장 다양한 데이터 타입 다루기
## 6.4 수치형 데이터 타입 다루기
- describe: 관련 컬럼에 대한 집계, 평균, 표준편차, 최솟값, 최댓값 계산
- crosstab: 자주 사용하는 항목 쌍을 확인하는 용도의 메서드
- monotonically_increasing_id: 모든 로우에 고유ID값 추가 -> 모든 로우에 0부터 시작하는 고윳값 생성

## 6.5 문자열 데이터 타입 다루기
- initcap: 주어진 문자열에서 공백으로 나뉘는 모든 단어의 첫 글자를 대문자로 변경
- lpad, rpad: 문자열의 길이보다 작은 숫자를 넘기면 문자열의 오른쪽부터 제거됨

### 6.5.1 정규표현식
- instr: 값의 존재 여부 확인
- locate: 문자열의 위치를 정수로 반환(위치는 1부터 시작)

## 6.6 날짜와 타임스탬프 데이터 타입 다루기
- 스파크는 두 종류의 시간 관련 정보만 집중적으로 관리
- date: 달력형태
- timestamp: 날짜와 시간
- inferSchema 옵션이 활성화된 경우 날짜와 타임스탬프를 포함해 컬럼의 데이터 타입을 최대한 정확하게 식별하려 시도
- 스파크는 특정 날짜 포맷을 명시하지 않아도 자체적으로 식별해 데이터를 읽을 수 있음
- TimestampType 클래스는 초 단위 정밀도까지 지원
- 밀리세컨드나 마이크로 세컨드 단위를 다룬다면 Long 데이터 타입으로 데이터 변환해 처리하는 우회정책을 사용해야 함
- datediff: 두 날짜 사이의 일 수 반환
- months_between: 두 날짜 사이의 개월 수 반환
- to_date: 문자열을 날짜로 변환, 필요에 따라 날짜 포맷 지정(반드시 자바의 SimpleDateFormat 클래스)
- 스파크는 날짜를 파싱할 수 없다면 에러 대신 null 반환

## 6.7 null 값 다루기
- 스파크에서는 빈 문자열이나 null 대체값 대신 null 값을 사용해야 최적화를 수행할 수 있음

### 6.7.1 coalesce
- coalesce 함수는 인수로 지정한 여러 컬럼 중 null이 아닌 첫번째 값을 반환
- 모든 컬럼이 null이 아닌 값을 가지는 경우 첫 번째 컬럼의 값을 반환

## 6.9 복합 데이터 타입 다루기
- 구조체, 배열, 맵

### 6.9.1 구조체
- 데이터프레임 내부의 데이터프레임
- 쿼리문에서 다수의 컬럼을 괄호로 묶어 구조체를 만들 수 있음
- 구조체의 컬럼을 조회하려면 getField 함수 이용

### 6.9.2 배열
- split: 배열로 변환할때 사용, 구분자를 인수로 전달해 배열로 변환
- explode: 복합 데이터 타입의 배열에 존재하는 모든 로우를 변환

### 6.9.3 맵
- 맵은 map함수와 컬럼의 키-값 쌍을 이용해 생성
- 적합한 키를 사용해 데이터를 조회할 수 있고 해당 키가 없다면 null 값을 반환

## 6.11 사용자 정의 함수
- 파이썬이나 스칼라, 외부 라이브러리를 사용해 사용자가 원하는 형태로 트랜스포메이션을 만들 수 있게 함
- 하나 이상의 컬럼을 입력으로 받고, 반환할 수 있음
- 레코드별로 데이터를 처리하는 함수이기 때문에 독특한 포맷이나 도메인에 특화된 언어를 사용하지 않음
- 기본적으로 특정 스파크세션이나 Context에서 사용할 수 있도록 임시 함수 형태로 등록됨
- 스칼라, 파이썬, 자바로 UDF 개발 가능
- 언어별로 성능에 영향을 미칠 수 있으므로 주의
- 스파크는 드라이버에서 함수를 직렬화하고 네트워크를 통해 모든 익스큐터 프로세스로 전달
- 파이썬으로 함수를 작성했다면 매우 다르게 동작함
  - 스파크는 워커노드에 파이썬 프로세스를 실행하고 파이썬이 이해할 수 있는 포맷으로 모든 데이터를 직렬화
  - 파이썬 프로세스에 있는 데이터의 로우마다 함수를 실행하고 마지막으로 JVM과 스파크에 처리 결과를 반환
  - 파이썬 프로세스를 시작하는 부하도 크지만 진짜 부하는 파이썬으로 데이터를 전달하기 위해 직렬화하는 과정에서 발생함
    - 작렬화에 큰 부하 발생
    - 데이터가 파이썬으로 전달되면 스파크에서 데워커 메모리를 관리할 수 없음 -> JVM과 파이썬이 동일한 머신에서 메모리 경합을 하면 자원에 제약이 생겨 워커가 비정상적으로 종료될 가능성이 있음
- 함수에서 반환될 실제 데이터 타입와 일치하지 않는 데이터 타입을 지정하면 스파크는 오류가 아닌 null 값을 반환함
- UDF에서 값을 선택적으러 반환하려면 파이썬을 None을 반환해야 함


# 7장 집계 연산
## 7.1 집계 함수
### 7.7.1 count
- count(*)를 사용하면 null 값을 가진 로우를 포함해 카운트
- count 함수에 특정 컬럼을 지정하면 null 값을 카운트하지 않음

### 7.1.3 approx_count_distinct
- 대규모 데이터셋을 다루다보면 정확한 고유 개수가 무의미하고 어느정도 수준의 정확도를 가지는 근사치만으로도 유의미할 때
- 최대 추정 오류율이라는 한가지 파라미터가 있음
  - 큰 오류율을 설정하면 기대치에서 크게 벗어나는 결과를 얻게 됨
  - 대신 countDistinct 함수보다 더 빠르게 결과를 반환
- 이 함수의 성능은 대규모 데이터셋을 사용할 때 훨씬 더 좋아짐

### 7.1.7 sumDistinct
- 고윳값의 합산

### 7.1.9 분산과 표준편차
- 스파크는 표본표준편차와 모표준편차 둘다 지원함
- variance나 stddev 함수를 사용한다면 기본적으로 표본표준분산과 표본표준편차 공식 이용
- var_pop나 stddev_pop는 모표준분산이나 모표준편차 방식 사용

### 7.4.1 롤업
- 다양한 컬럼을 그룹화 키로 설정하면 그룹화 키로 설정된 조합뿐만 아니라 데이터셋에서 볼 수 있는 실제 조합을 모두 살펴볼 수 있음
- 롤업은 그룹바이 스타일의 다양한 연산을 수행할 수 있는 다차원 집계 기능

### 7.4.2 큐브
- 큐브는 롤업을 고차원적으로 사용할 수 있게 해줌
- 큐브는 요소들을 계층적으로 다루는 대신 모든 차원에 대해 동일한 작업을 수행함

### 7.4.3 그룹화 메타데이터
- group id 사용: 큐브와 롤업을 사용하다 보면 집계 수준에 따라 쉽게 필터링하기 위해 집계 수준을 조회하는 경우 발생할 때 사용
- ㄴ 결과 데이터셋의 집계 수준을 명시하는 컬럼을 제공

### 7.4.4 피벗
- 데이터를 탐색하는 방식에 따라 피벗을 수행한 결괏값이 감소할 수도 있음
- 특정 컬럼의 카디널리티가 낮다면 스키마와 쿼리 대상을 확인할 수 있도록 피벗을 사용해 다수의 컬럼으로 변환하는 것이 좋음

## 7.5 사용자 정의 집계 함수
- 스파크는 입력 데이터의 모든 그룹의 중간 결과를 단일 AggregationBuffer에 저장해 관리
- UDAF를 생성하려면 기본 클래스인 UserDefinedAggregateFunction을 상속받음

# 8장 조인
## 8.1 조인 표현식
- 스파크는 왼쪽과 오른쪽 데이터셋에 있는 하나 이상의 키값을 비교하고 왼쪽 데이터셋과 오른쪽 데이터셋의 결합 여부를 결정하는 조인 표현식의 평가 결과에 따라 조인함
- 가장 많이 사용되는 조인 표현식은 왼쪽과 오른쪽 데이터셋에 지정된 키가 동일한지 비교하는 동등 조인(equi-join)
- 스파크는 키가 일치하지 않으면 데이터셋을 결합하지 않음
- 스파크는 일치하는 키가 없는 로우는 조인에 포함시키지 않음
- 스파크는 동등 조인뿐만 아니라 더 복잡한 조인 정책도 지원
- 복합 데이터 타입을 조인에 사용할 수 있음

## 8.7 왼쪽 세미 조인
- 세미 조인은 오른쪽 데이터프레임의 어떤 값도 포함하지 않기 때문에 다른 조인 타입과는 약간 다름
- 두번째 데이터프레임은 값이 존재하는지 확인하기 위해 값만 비교하는 용도로 사용
- 만약 값이 존재한다면 왼쪽 데이터프레임에 중복 키가 존재하더라도 해당 로우는 결과에 포함된
- 왼쪽 세미 조인은 기존 조인 기능과 달리 데이터프레임의 필터 정도로 볼 수 있음

## 8.8 왼쪽 안티 조인
- 왼쪽 세미 조인의 반대 개념
- 왼쪽 세미 조인처럼 오른쪽 데이터프레임의 어떤 값도 포함하지 않음
- 단지 두번째 데이터프레임은 값이 존재하는지 확인하기 위해 값만 비교하는 용도로 사용함
- 두번째 데이터프레임에서 관련된 키를 찾을 수 없는 로우만 결과에 포함됨
- 안티 조인은 SQL의 NOT IN과 같은 스타일의 필터로 생각할 수 있음

## 8.9 자연 조인
- 조인하려는 컬럼을 암시적으로 추정
- 일치하는 컬럼을 찾고 그 결과를 반환함

## 8.10 교차 조인(카테시안 조인)
- 교차조인은 조건절을 기술하지 않은 내부 조인을 의미
- 교차조인은 왼쪽 데이터프레임의 모든 로우를 오른쪽 데이터 프레임의 모든 로우와 결합
- 교차조인을 거치면 엄청난 수의 로우를 가질 수 있으므로 반드시 키워드를 이용해 교차 조인을 수행한다는 것을 명시적으로 선언해야 함

## 8.11 조인 사용 시 문제점
### 8.11.1 복합 데이터 타입의 조인
- 불리언을 반환하는 모든 표현식은 조인 표현식으로 간주할 수 있음

### 8.11.2 중복 컬럼명 처리
- 문제 상황
  - 조인에 사용할 데이터프레임의 특정 키가 동일한 이름을 가지며, 키가 제거되지 않도록 조인 표현식에 명시하는 경우
  - 조인 대상이 아닌 두 개의 컬럼이 동일한 이름을 가진 경우
- 해결 방법
1. 다른 조인 표현식 사용
   - 동일한 이름을 가진 두 개의 키를 사용한다면 불리언 형태의 조인 표현식을 문자열이나 시퀀스 형태로 바꾸는 것
2. 조인 후 컬럼 제거
   - 이 방법은 스파크 SQL 분석 프로세스의 특성을 활용
   - 명시적으로 참조된 컬럼을 검증할 필요가 없으므로 스파크 코드 분석 단계를 통과함
3. 조인 전 컬럼명 변경

## 8.12 스파크의 조인 수행 방식
### 8.12.1 네트워크 통신 전략
- 스파크는 조인 시 두가지 클러스터 통신 방식을 활용
  - 셔플조인: 전체 노드 간 통신 유발
  - 브로드캐스트 조인: 전체 노드 간 통신을 유방하지는 않음
- 큰 테이블과 큰 테이블 조인
  - 셔플 조인 발생 -> 셔플 조인은 전체 노드 간 통신 발생, 조인에 사용한 특정 키나 집합을 어떤 노드가 가졌는지에 따라 해당 노드와 데이터를 공유
  - 이런 통신 방식 때문에 네트워크는 복잡해지고 많은 자원을 사용, 특히 데이터가 잘 나뉘어져 있지 않다면 더 심해짐
- 전체 조인 프로세스가 진행되는 동안 모든 워커노드에서 통신이 발생함
- 큰 테이블과 작은 테이블 조인
  - 테이블이 단일 워커 노드의 메모리 크기에 적합할 정도로 충분히 작은 경우 조인 연산 최적화 가능
  - 브로드캐스트 조인이 효율적 -> 브로트캐스트 조인은 작은 데이터프레임을 클러스터의 전체 워커노드에 복제하는 것을 의미
  - 이렇게하면 자원을 많이 사용할 것처럼 보이지만 조인 프로세스 내내 전체 노드가 통신하는 현상을 방지할 수 있음
  - 시작 시 단 한번만 복제가 수행되며 그 이후로는 개별 워커가 다른 워커노드를 기다리거나 통신할 필요 없이 수행 가능
  - 브로드캐스트 조인도 대규모 노드 간 통신이 발생하지만 그 이후로는 노드 사이에 추가적인 통신이 발생하지 않음
  - 따라서 모든 단일 노드에서 개별적으로 조인이 수행되므로 CPU가 가장 큰 병복 구간이 됨
  - 데이터프레임 API를 사용하면 옵티마이저에서 브로드캐스트 조인을 사용할 수 있도록 힌트를 줄 수 있음
  - 힌트를 주는 방법은 브로드캐스트 함수에 작은 크기의 데이터프레임을 인수로 전달
  - SQL 역시 조인 수행에 필요한 힌트를 줄 수 있지만 강제성이 없으므로 옵티마이저가 무시할수도 있음
  - 단점: 너무 큰 데이터를 브로드캐스트하면 고비용의 수집 연산이 발생하므로 드라이버 노드가 비정상적으로 종료될 수도 있음
- 아주 작은 테이블 사이의 조인
  - 스파크가 조인 방식을 경정하도록 내버려두는 것이 좋음


# 9장 데이터소스
### 9.1.2 데이터 읽기의 기초
- 읽기모드: 스파크가 형식에 맞지 않는 데이터를 만났을 때 동작 방식을 지정하는 옵션
  - permissive(기본값): 오류 레코드의 모든 필드를 null로 설정하고 모든 오류 레코드를 _corrupt_record라는 문자열 컬럼에 기록
  - dropMalformed: 형식에 맞지 않는 레코드가 포함된 로우 제거
  - failFast: 형식에 맞지 않는 레코드를 만나면 즉시 종료

### 9.1.3 쓰기 API 구조
- partitionBy, bucketBy, sortBy 메서드는 파일 기반 데이터소스에서만 동작, 이 기능으로 최종 파일 배치 형태 제어 가능

### 9.1.4 데이터 쓰기의 기초
- 저장모드: 스파크가 지정된 위치에서 동일한 파일을 발견했을 때의 동작 방식을 지정하는 옵션
  - append: 해당 경로에 이미 존재하는 파일 목록에 결과 파일 추가
  - overwrite: 이미 존재하는 모든 데이터를 완전히 덮어 씀
  - errorIfExists(기본값): 해당 경로에 데이터나 파일이 존재하는 경우 오류를 발생시키면서 쓰기 작업이 실패함
  - ignore: 해당 경로에 데이터나 파일이 존재하는 경우 아무런 처리도 하지 않음

### 9.2.2 CSV 파일 읽기
- 스파크는 지연 연산 특성이 있으므로 데이터프레임 정의 시점이 아닌 잡 실행 시점에만 오류가 발생
- ex. 데이터프레임을 정의하는 시점에는 존재하지 않는 파일을 지정하도 오류가 발생하지 않음

## 9.4 파케이 파일
- 분석 워크로드에 최적화
- 저장소 공간 절약, 전체 파일을 읽는 대신 개별 컬럼을 읽을 수 있음, 컬럼 기반 압축 기능 제공
- 스파크와 호환이 잘 되어 스파크 기본 파일 포맷
- 파케이 파일은 읽기 연산 시 JSON, CSV보다 훨씬 효율적으로 동작하므로 장기 저장용 데이터는 파케이 포맷으로 저장하는 것이 좋음
- 복합 데이터 타입 지원 -> 컬럼이 배열, 맵, 구조체 데이터 타입이어도 문제 없음
- cf. CSV에서는 배열을 사용할 수 없음