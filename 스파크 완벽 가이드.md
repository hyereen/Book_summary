

# 하둡 완벽 가이드

# Part 1: 빅데이터와 스파크 간단히 살펴보기
## 1장 아파치 스파크란
- 아파치 스파크: 통합 컴퓨팅 엔진, 클러스터 환경에서 데이터를 병렬로 처리하는 라이브러리 집합

### 1.1 아파치 스파크의 철학
- 통합: 스파크는 간단한 데이터 읽기에서부터 SQL 처리, 머신러닝, 스트림 처리를 같은 연산 엔진과 일관성 있는 API로 수행할 수 있도록 설계
- 컴퓨팅 엔진
  - 저장소 시스템의 데이터를 연산하는 역할만 수행하고 영구 저장소 역할은 수행하지 않음
  - 스파크는 내부에 데이터를 오랜 시간 저장하지 않고 특정 저장소 시스템을 선호하지 않음

## 2장 스파크 간단히 살펴보기
### 2.1 스파크의 기본 아키텍처
#### 2.1.1 스파크 애플리케이샨
- 드라이버 프로세스
  - 클러스터 도느 중 하나에서 실행됨
  - main함수를 실행함
  - 스파크 애플리케이션 정보의 유지 관리, 사용자 프로그램이나 입력에 대한 응답, 전반적인 익스큐터 프로세스의 작업과 관련된 분석, 배포, 스케줄링 역할
  - 스타크 애플리케이션의 수명 주기 동안 관련 정보 모두 유지
  - 주어진 작업을 완료하기 위해 드라이버 프로그램의 명령을 익스큐터에서 실행할 책임이 있음
- 익스큐터 프로세스
  - 드라이버 프로세스가 할당한 작업을 수행
  - 드라이버가 할당한 코드를 실행하고 진행상황을 다시 드라이버 노드에 보고하는 역할 수행
  - 스파크 코드를 실행하는 역할
- 클러스터 매니저
  - 스파크 스탠드얼론 클러스터 매니저, 하둡 YARN, 메소스 중 하나 선택 가능
  - 하나의 클러스터에서 여러 개의 스파크 애플리케이션 실행 가능
  - 스파크는 사용가능한 자원을 파악하기 위해 클러스터 매니저를 사용

### 2.2 스파크의 다양한 언어 API
- 사용자는 스파크 코드를 실행하기 위해 SparkSession 객체를 진입점으로 사용할 수 있음
- 파이썬이나 R로 스파크를 사용할 때는 JVM 코드를 명시적으로 작성 X
- 스파크는 사용자를 대신해 파이썬이나 R로 작성한 코드를 익스큐터의 JVM에서 실행할 수 있는 코드로 변환함

### 2.3 스파크 API
- 다양한 언어로 스파크를 사용할 수 있는 이유는 스파크가 기본적으로 두가지 API를 제공하기 때문
  - 저수준 비구조적 API
  - 고수준 구조적 API

### 2.5 SparkSession
- 스파크 애플리케이션은 SparkSession이라고 불리는 드라이버 프로세스를 제어함
- SparkSession 인스턴스
  - 사용자가 정의한 처리 명령을 클러스터에서 실행
  - 하나의 SparkSession은 하나의 스파크 애플리케이션에 대응

### 2.6 DataFrame
- 가장 대표적인 구조적 API
- 스키마: 컬럼과 컬럼의 타입을 정의한 목록
- 스파크 DataFrame은 수천 대의 컴퓨터에 분산되어 있음
- Pandas나 R의 DataFrame에는 분산 컴퓨터가 아닌 단일 컴퓨터에 존재함 -> 데이터프레임으로 수행할 수 있는 작업이 해당 머신이 가진 자원에 따라 제한될 수 밖에 없음

### 2.6.1 파티션
- 스파크는 모든 익스큐터가 병렬로 작업을 수행할 수 있도록 파티션이라 불리는 청크 단위로 데이터를 분할함
- 파티션: 클러스터의 물리적 머신에 존재하는 로우의 집합을 의미
- 데이터프레임의 파티션은 실행 중에 데이터가 컴퓨터 클러스터에서 물리적으로 분산되는 방식을 나타냄
  - 만약 파티션이 하나라면 스파크의 수천 개의 익스큐터가 있더라도 병렬성은 1이 됨
  - 수백 개의 파티션이 있더라도 익스큐터가 하나 밖에 없다면 병렬성은 1이됨
- 데이터프레임을 사용하면 파티션을 수동 혹은 개별적으로 처리할 필요가 없음
- 물리적 파티션에 데이터 변환용 함수를 지정하면 스파크가 실제 처리 방법을 결정함

## 2.7 트랜스포메이션
- 스파크 핵심 데이터 구조는 불변성 -> 한번 생성하면 변경할 수 없음
- 데이터프레임을 변경하려면 원하는 변경 방법을 스파크에게 알려줘야 함 -> 이때 사용하는 명령을 트랜스포메이션이라고 함
- 변수에 값을 저장하는 코드를 실행해도 결과는 출력되지 않음 -> 추상적인 트랜스포메이션만 지정한 상태이기 때문에 액션을 호출하지 않으면 스파크는 실제 트랜스포메이션을 실행하지 않음
- 트랜스포메이션 유형
  - 좁은 의존성
    - 각 입력 파티션이 하나의 출력 파티션에만 영향을 미침
    - where 구문은 좁은 의존성을 가짐 
    - 하나의 파티션이 하나의 출력 파티션에만 영향을 미침
    - 스파크에서 파이프라이닝을 자동으로 수행 -> 데이터프레임에 여러 필터를 지정하는 경우 모든 작업이 메모리에서 일어남
  - 넓은 의존성
    - 하나의 입력 파티션이 여러 출력 파티션에 영향을 미침
    - 클러스터에서 파티션을 교환하는 셔플이 동작하여 셔플의 결과를 디스크에 저장함

### 2.7.1 지연 연산
- 지연 연산 lazy evaluation: 스파크가 연산 그래프를 처리하기 직전까지 기다리는 동작 방식을 의미
- 스파크는 특정 연산 명령이 내려진 즉시 데이터를 수정하지 않고 원시 데이터에 적용할 트랜스포메이션의 실행 계획을 생성함
- 스파크는 코드를 실행하는 마지막 순간까지 대기하다가 원형 데이터프레임 트랜스포메이션을 간결한 물리적 실행 계획으로 컴파일함
- 스파크는 이 대기 과정을 거치며 전체 데이터 흐름을 최적화하는 엄청난 강점을 가지고 있음
- 예시: 데이터 프레임의 조건절 푸시다운 predicate pushdown
- 아주 복잡한 스파크 잡이 원시 데이터에서 하나의 로우만 가져오는 필터를 가지고 있다면 필요한 레코드 하나만 읽는 것이 효율적 -> 스파크는 이 필터를 데이터소스로 위임하는 최적화 작업을 자동을 수행

## 2.8 액션
- 사용자는 트랜스포메이션을 사용해 논리적 실행 계획을 세울 수 있음
- 하지만 실제 연산을 수행하려면 액션 명령을 내려야 함
- 액션은 일련의 트랜스포메이션으로부터 결과를 계산하도록 지시하는 명령
- 액션 유형
  - 콘솔에서 데이터를 보내는 액션
  - 각 언어로 된 네이티브 객체에 데이터를 모으는 액션
  - 출력 데이터소스에 저장하는 액션
- 액션을 지정하면 스파크 잡이 시작됨
- 스파크 잡은 필터(좁은 트랜스포메이션)를 수행한 후 파티션별로 레코드 수를 카운트(넓은 트랜스포메이션)함
- 그리고 각 언어에 적합한 네이티브 객체를 모음
- 이때 스파크가 제공하는 스파크 UI로 클러스터에서 실행중인 스파크 잡을 모니터링할 수 있음

## 2.9 스파크UI
- 스파크 잡의 진행 상황을 모니터링할 때 사용
- 드라이버 노드의 4040 포트로 접속 가능
- 스파크 잡 상태, 환경 설정, 클러스터 상태 등 확인
- 스파크 잡을 튜닝하고 디버깅할 때 매우 유용
- 스파크 잡은 개별 액션에 의해 트리거되는 다수의 트랜스포메이션으로 이루어져 있으며 스파크 UI로 잡을 모니터링할 수 있음

### 2.10.1 Dataframe과 SQL
- 스파크는 언어 상관없이 같은 방식으로 트랜스포메이션을 실행할 수 있음
- 스파크는 SQL쿼리를 데이터프레임 코드와 같은 실행 계획으로 컴파일하므로 둘 사이의 성능의 차이가 없음
- 스파크는 해당 데이터프레임이나 자신의 원본 데이터프레임에 액션이 호출되기 전까지 데이터를 읽지 않음

# 3장 스파크 기능 둘러보기
## 3.1 운영용 애플리케이션 실행하기
- spark-submit 명령
  - 대화형 셸에서 개발한 프로그램을 운영용 애플리케이션으로 쉽게 전환 가능
  - 애플리케이션 코드를 클러스터에 전송해 실행시키는 역할
  - 애플리케이션 실행에 필요한 자원과 실행 방식, 옵션을 지정 가능
  - master 옵션의 인숫값을 변경하면 스파크가 지원하는 스파크 스탠드얼론, 메소스, YARN 클러스터 매니저에서 동일한 애플리케이션 실행 가능

## 3.2 Dataset: 타입 안정성을 제공하는 구조적 API
- Dataset
  - 자바와 스칼라의 정적 데이터 타입에 맞는 코드 -> 정적 타입 코드를 지원하기 위해 고안된 스파크의 구조적 API
  - 타입안정성을 지원하며 동적타입 언어인 파이썬과 R에서는 사용 X
  - 데이터프레임의 레코드를 사용자가 자바나 스칼라로 정의한 클래스에 할당하고 자바의 ArrayList 또는 스칼라의 Seq 객체 등의 고정 타입형 컬렉션으로 다룰 수 있는 기능 제공
  - 장점
    - 필요한 경우에 선택적으로 사용 가능
    - 스파크가 제공하는 여러 함수를 이용해 추가 처리 작업 가능
    - collect 메서드나 take 메서드를 호출하면 Dataset에 매개변수로 지정한 타입의 객체를 반환
    - 코드 변경 없이 타입 안정성을 보장, 로컬이나 분산 클러스터 환경에서 데이터를 안전하게 다룰 수 있음

## 3.3 구조적 스트리밍
- 구조적 스트리밍: 스파크 2.2 버전에서 안정화된 스트림 처리용 고수준 API
- ㄴ 사용하면 구조적 API로 개발된 배치 모드의 연산을 스트리밍 방식으로 실행 가능, 지연 시간을 줄이고 증분 처리 가능
- 배치 처리용 코드를 일부 수정하여 스트리밍 처리를 수행하고 값을 빠르게 얻을 수 있다는 장점

## 3.4 머신러닝과 고급 분석
- 스파크에서 머신러닝 모델을 학습시키는 과정
  1. 아직 학습되지 않은 모델 초기화 -> 학습 전 알고리즘 명칭: Algorithm
  2. 해당 모델 학습: AlgorithmModel

# 4장 구조적 API 개요
## 4.1 DataFrame과 Dataset
- 스파크는 DataFrame과 Dataset이라는 두 가지 구조화된 컬렉션 개념을 가지고 있음
  - 잘 정의된 로우와 컬럼을 가지는 분산 테이블 형태의 컬렉션
  - 각 컬럼은 다른 컬러과 동일한 수의 로우를 가져야 함
  - 컬렉션의 모든 로우는 같은 데이터 타입 정보를 가지고 있어야 함
  - 결과를 생성하기 위해 어떤 데이터에 어떤 연산을 적용해야 하는지 정의하는 지연 연산의 실행 계획, 불변성을 가진
- DataFrame에 액션을 호출하면 스파크는 트랜스포메이션을 실제로 실행하고 결과를 반환함
- 스키마는 분산 컬렉션에 저장할 데이터 타입을 정의하는 방법

## 4.2 스키마
- DataFrame의 컬럼명과 데이터 타입을 정의
- 스키마는 데이터 소스에서 얻거나(Schema on read) 직접 정의
- 스키마는 여러 데이터 타입으로 구성되므로 어떤 타입이 어느 위치에 있는지 정의하는 방법 필요

## 4.3 스파크의 구조적 데이터 타입 개요
- 스파크는 실행 계획 수립과 처리에 사용하는 자체 데이터 타입 정보를 가지고 있는 카탈리스트 엔진을 사용함
  - 카탈리스트 엔진은 다양한 실행 최적화 기능을 제공
- 스파크는 자체 데이터 타입을 지워하는 여러 언어 API와 직접 매핑되며, 각 언어에 대한 매핑 테이블을 가지고 있음

### 4.3.1 Dataframe과 Dataset 비교
- Dataframe
  - 비타입형
  - 스키마에 명시된 데이터 타입의 일치 여부를 런타임이 되어서 확인
  - Row 타입으로 구성된 Dataset
    - Row타입은 스파크가 사용하는 연산에 최적화된 인메모리 포맷의 내부적인 표현 방식
    - Row타입을 사용하면 가비지 컬렉션과 객체 초기화 부하가 있는 JVM 데이터 타입을 사용하는 대신 자체 데이터 포맷을 사용하기 때문에 매우 효율적인 연산 가능
    - 파이썬이나 R에서는 스파크의 Dataset을 사용할 수 없지만 최적화된 포맷인 Dataframe 사용 가능
  - Dataframe을 사용하면 스파크의 최적화된 내부 포맷 사용 가능 -> 내부 포맷 사용하면 스파크가 지원하는 어떤 언어 API를 사용하더라도 동일한 효과와 효율성을 얻을 수 있음
- Dataset
  - 타입형
  - 스키마에 명시된 데이터 타입의 일치 여부를 컴파일 타임에 확인
  - JVM 기반의 언어인 스칼라와 자바에서만 지원
  - 데이터 타입을 정의하려면 스칼라의 케이스 클래스나 자바 빈을 사용해야 함

### 4.3.2 컬럼
- 컬럼은 정수형,문자열 같은 단순 데이터 타입, 배열,맵 같은 복합 데이터 타입, null 값을 표현

### 4.3.3 로우
- 로우는 데이터 레코드
- Dataframe의 레코드는 Row타입으로 구성된
- 로우는 SQL, RDD, 데이터소스에서 얻거나 직접 만들 수 있음 

## 4.4 구조적 API의 실행 과정
- 구조적 API 쿼리가 사용자 코드에서 실제 실행 코드로 변환되는 과정
1. DataFrame/Dataset/SQL을 이용해 코드를 작성
2. 정상적인 코드라면 스파크가 논리적 실행계죅으로 변환
3. 스파크는 논리적 실행계획을 물리적 실행 계획으로 변환하며 그 과정에서 추가적인 최적화를 할 수 있는지 확인
4. 스파크는 클러스터에서 물리적 실행 계획(RDD처리)을 실행