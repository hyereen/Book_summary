# PART 1 기본편

## CHAPTER 1 Apache Airflow 살펴보기
### p.4 1.1 데이터 파이프라인 소개
- 방향성 비순환 그래프 Directed Acyclic Graph, DAG
  - 그래프는 화살표 방향성의 끝점을 포함하되 반복이나 순환을 허용하지 않음(비순환)
  - 비순환 속성은 태스크 간의 순환 실행을 방지하기 때문에 매우 중요함

#### p.9 1.1.4 워크플로 매니저를 이용한 파이프라인 실행
- 워크플로 매니저마다 워크플로 정의 방식의 차이가 있음
  - Oozie는 정적 파일(XML)을 사용하여 워크플로를 정의하기 때문에 읽기 쉽지만 유연하지 않음
  - Luigi와 Airflow는 코드로 워크플로 정의 가능 -> 좀 더 유연하지만 읽기 및 테스트는 어려울 수 있음(워크플로 구현하는 코딩 능력에 따라 잘라짐)
- 워크플로 관리자가 제공하는 기능 범위
  - Make나 Luigi는 워크플로 스케줄을 위한 기본 기능 제공 X -> 반복 스케줄로 워크플로를 실행하기 위해서는 Cron과 같은 추가 도구를 사용해야 함

### p.10 1.2 Airflow 소개
#### p.10 1.2.1 파이썬 코드로 유연한 파이프라인 정의
- 파이프라인이나 워크플로 태스크를 방향성 비순환 그래프(DAG)로 정의 가능
- 태스크는 노드로 정의하고 의존성은 태스크 노드 사이에 화살표로 정의함
- 파이썬 스크립트로 DAG 구조를 설명하고 구상함
- 일반적으로 DAG 파일은 주어진 DAG에 대한 태스크 집합과 태스크 간의 의존성을 기술하고, Airflow는 DAG 구조를 식별하기 위해 코드를 파싱함
- Airflow DAG를 파이썬 코드로 정의하여 많은 유연성 제공 가능

#### p.11 1.2.2 파이프라인 스케줄링 및 실행
- Airflow 주요 구성 요소
  - Airflow 스케줄러: DAG를 분석하고 현재 시점에서 DAG의 스케줄이 지난 경우 Airflowㅇ 워커에 DAG의 태스크를 예약
  - Airflow 워커: 예약된 태스크를 선택하고 실행함
  - Airflow 웹서버: 스케줄러에서 분석한 DAG를 시각화하고 DAG 실행과 결과를 확인할 수 있는 주요 인터페이스 제공

#### p.16 1.2.4 점진적 로딩 및 백필
- Airflow의 스케줄 기능 중 강력한 장점
  - DAG에 정의된 특정 시점에 트리거할 수 있음
  - 최종 시점과 예상되는 다음 스케줄 주기를 상세하게 알려주는 것 -> 각각의 주기로 나누고 각 주기별로 DAG 실행 가능
- Airflow의 특성은 데이터 파이프라인을 점진적으로 실행할 수 있도록 구성이 가능하기 때문에 효율적인 데이터 파이프라인 구축 가능
- 점진적인 파이프라인에서 각 DAG는 매번 전체 데이터 세트를 다시 처리할 필요 없이 해당 시간 슬롯에 대한 데이터만 처리 가능
- 대규모 데이터 세트를 처리해야 할 경우, 기존 결과에 대한 태스크 전체를 다시 수행하는 것을 방지하여 많은 시간, 비용 절감 가능
- 스케줄 주기가 백필 개념과 결합하여 더욱 강력하게 활용 가능
  - 이를 통해 새로 생성한 DAG를 과거 시점 및 기간에 대해 실행 가능
  - 과거 특정 기간에 대해 DAG를 실행해 새로운 데이터를 손쉽게 생성 (또는 백필)할 수 있음
  - 과거 실행 결과를 삭제한 다음, 태스크 코드를 변경한 후에 삭제된 과거 태스크를 쉽게 재실행할 수 있기 때문에 필요할 때 전체 데이터 세트를 간단하게 재구성해 처리 가능

### p.16 1.3 언제 Airflow를 사용해야 할까
#### p.17 1.3.1 Airflow를 선택하는 이유
- 파이썬 코드를 이용해 파이프라인 구현 -> 복잡한 커스텀 파이프라인 구현 가능
- 다양한 시스템과의 통합 가능
- 수많은 스케줄링 기법 -> 파이프라인 정기적 실행하고 점진적 증분 처리를 통해 전체 파이프라인을 재실행할 필요 없는 효율적인 파이프라인 구축 가능
- 백필 기능 -> 과거 데이터 손쉽게 재처리 할 수 있기 때문에 코드를 변경한 후 재생성이 필요한 데이터 재처리 가능
- Airflow 웹 인터페이스 -> 파이프라인 실행 결과 모니터링 가능, 오류 디버깅을 위한 뷰 제공
- 오픈 소스

#### p.17 1.3.2 Airflow가 적합하지 않은 경우
- Airflow는 반복적이거나 배치 태스크를 실행하는 기능에 초점이 맞춰져 있기 때문에, 스트리밍 워크플로 및 파이프라인 처리에는 적합하지 않을 수 이음
- 추가 및 삭제 태스크가 빈번한 동적 파이프라인의 경우 적합하지 않을 수 있음 -> Airflow는 동적 태스크를 구현할 수 있지만, 웹 UI는 DAG의 가장 최근 실행 버전에 대한 정의만 표현해줌. 따라서 Airflow는 실행되는 동안 구조가 변경되지 않은 파이프라인에 좀 더 적합함
- 파이썬 코드로 DAG를 작성하는 것은 파이프라인 규모가 커지면 굉장히 복잡해질 수 있음 -> 초기 사용 시점부터 엄격한 관리 필요


## CHAPTER 2 Airflow DAG의 구조
### p.23 첫 번째 Airflow DAG 작성
#### p.27 2.1.1 태스크와 오퍼레이터 차이점 
- 오퍼레이터
  - 단일 작업 수행 역할
  - BashOperator: 배시 스크립트를 실행하는 데 사용됨
  - PythonOperator: 파이썬 함수를 실행하는 데 사용됨
  - SimpleHTTPOperator: HTTP 엔드포인트 호출
- DAG
  - 오퍼레이터 집합에 대한 실행을 오케스트레이션(조정, 조율)하는 역할
  - 오퍼레이터의 시작과 정지, 오퍼레이터가 완료되면 다음 태스크의 시작, 그리고 오퍼레이터 간 의존성 보장됨
- 태스크
  - 작업의 올바른 실행을 보장하기 위한 오퍼레이터의 wrapper 또는 manager로 생각해볼 수 있음
  - 오퍼레이터의 상태를 관리하고 사용자에게 상태 변경(예: 시작/완료)를 표시하는 에어플로우의 내장 컴포넌트

### p.36 2.4 스케줄 간격으로 실행하기
- DAG를 일정 시간 간격으로 실행할 수 있도록 스케줄 설정 가능 -> schedule_interval 인수 설정
- @daily로 설정하면 하루에 한번 실행

#### p.45 Cron 기반의 스케줄 간격 설정하기
- cron(macOS 및 리눅스와 같은 유닉스 기반 OS에서 사용하는 시간 기반 작업 스케줄러)과 동일한 구문을 사용해 스케줄 간격 정의 가능
- cron job은 시간/날짜가 해당 필드의 값과 시스템 시간이 일치할 때 실행됨
  - 0 * * * * = 매 시간(정시에 실행)
  - 0 0 * * * = 매일(자정에 실행)
  - 0 0 * * 0 = 메주(일요일 자정에 실행)
- 콤마를 사용하여 값의 리스트를 정의하거나 대시를 사용하여 값의 범위를 정의하는 값의 집합을 지정할 수 있음
  - 0 0 * * MON,WED,FRI = 매주 월, 화, 금 자정에 실행
- cron 식은 기능이 좋지만 작업하기 쉽지 않으므로 적용 전 작성된 태스크에 대해 테스트하는 것이 필요

#### p.47 3.2.3 빈도 기반의 스케줄 간격 설정하기
- cron 식의 제약은 특정 빈도마다 스케줄을 정의할 수 있다는 것
- 이러한 제약은 cron 식의 특성에 있음 -> 작업의 실행 여부를 결정하기 위해 패턴과 일치하는 현재 시간을 지속적으로 확인하기 위한 정의
- Airflow은 이런 상대적인 시간 간격으로 스케줄 간격을 정의할 수 있도록 정의
- 빈도 기반 스케줄을 사용하려면 timedelta인스턴스를 사용하면 됨

#### p.49 3.3.2 실행 날짜를 사용하여 동적 시간 참조하기
- execution_date: DAG가 실행되는 날짜와 시간을 나타냄
  - DAG를 시작하는 시간의 특정 날짜가 아니라 스케줄 간격으로 실행되는 시작 시간을 나타내는 타임스탬프
  - 종종 날짜를 형식화된 문자열로 참조하여 사용됨 -> Airflow는 일반적인 날짜 형식에 대한 여러 유형의 축약 매개변수를 제공
- next_execution_dat: 스케줄 간격의 종료 시간 -> 태스크의 스케줄 간격 정의
- previous_execution_date: 과거 스케줄 정의
  - 현재 시간 간격의 데이터와 이전 간격의 데이터를 대조하여 분석을 수행할때 유용함

#### p.51 3.3.3 데이터 파티셔닝
- 새로운 fetch_events 태스크로 이벤트 데이터를 새롭게 스케줄한 간격에 맞춰 점진적으로 가져오지만, 각각의 새로운 태스크가 전일의 데이터를 덮어쓰게 됨
  - 문제 해결 방법: events.json 파일에 새 이벤트를 추가하는 것 -> 하나의 JSON 파일에 모든 데이터 작성 가능
    - 단점: 특정 날짜의 통계 계산을 하려고 해도 전체 데이터 세트를 로드하는 다운스트림 프로세스 작업이 필요, 장애 지점이 되어 파일이 손상되어 전체 데이터 세트가 손실될 위험을 가지게 됨
- 태스크의 출력을 해당 실행 날짜의 이름이 적힌 파일에 기록함으로써 데이터 세트를 일일 배치로 나누는 것

### p.53 3.4 Airflow 실행 날짜 이해
#### p.53 3.4.1 고정된 스케줄 간격으로 태스크 실행
- 시작 날짜, 스케줄 간격 및 종료 날짜로 DAG 실행 시점 제어 가능
- 실제로 DAG를 예약하기 위해 위 세 가지 매개변수를 사용해 시간을 스케줄 간격으로 나눔
- 간격 기반 접근 방식: 해당 간격의 시간 슬롯이 경과되자마자 해당 간격 동안 DAG가 실행됨
  - 이점: 작업이 실행되는 시간 간격을 정확히 알고 있으므로 증분 데이터 처리 유형을 처리하는데 적합
    - 이는 작업이 실행되는 현재 시점의 시간만 아는 cron과 같은 시점 기반 스케줄링 시스템과 극명한 대조를 이룸